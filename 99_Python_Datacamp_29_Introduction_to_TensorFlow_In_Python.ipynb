{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to TensorFlow in Python Datacamp\n",
    "\n",
    "https://campus.datacamp.com/courses/introduction-to-tensorflow-in-python/introduction-to-tensorflow?ex=1#_=_\n",
    "\n",
    "\n",
    "##### 01 - Introduction to Tensor Flow\n",
    "\n",
    "Before you can build advanced models in TensorFlow 2.0, you will first need to understand the basics. In this chapter, you’ll learn how to define constants and variables, perform tensor addition and multiplication, and compute derivatives. Knowledge of linear algebra will be helpful, but not necessary. <br>\n",
    "constants and variables, defining data as constants, defining variables, basic operations, performing element-wise multiplication, making predictions with matrix multiplication,summing over tensor dimenstions\n",
    "##### 02 - Linear Models\n",
    "\n",
    "In this chapter, you will learn how to build, solve, and make predictions with models in TensorFlow 2.0. You will focus on a simple class of models – the linear regression model – and will try to predict housing prices. By the end of the chapter, you will know how to load and manipulate data, construct loss functions, perform minimization, make predictions, and reduce resource use with batch training.<br>\n",
    "input data, load data using pandas, setting the data type, loss function, loss function in TensorFlow,modifying the loss function, linear regression, set up linear regression, train a linear model, multiple linear regression, batch training, preparing to batch train, training a linear model in batches\n",
    "###### 03 -Neural Nerworks\n",
    "\n",
    "The previous chapters taught you how to build models in TensorFlow 2.0. In this chapter, you will apply those same tools to build, train, and make predictions with neural networks. You will learn how to define dense layers, apply activation functions, select an optimizer, and apply regularization to reduce overfitting. You will take advantage of TensorFlow's flexibility by using both low-level linear algebra and high-level Keras API operations to define and train models.\n",
    "Dense layers, the linear algebra of dense layers, the low-level approach with multiple examples, \n",
    "using the dense layer operation, activation functions, binary classification problems, multiclass classification problems, optimizers, the dangers of local minima, training a network in TensorFlow, Defining the model and loss function, Training neural networks with TensorFlow\n",
    "\n",
    "###### 04 - High Level APIs\n",
    "\n",
    "\n",
    "In the final chapter, you'll use high-level APIs in TensorFlow 2.0 to train a sign language letter classifier. You will use both the sequential and functional Keras APIs to train, validate, make predictions with, and evaluate models. You will also learn how to use the Estimators API to streamline the model definition and training process, and to avoid errors.\n",
    "\n",
    "Defining a multiple input model, training and validation with Keras, Training with Keras, Metrics and validation with Keras, Overfitting models, Training models with the Estimators API, preparing to train with Estimators, Defining Estimators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 01 - Introduction to Tensor Flow\n",
    "##### constants and variables\n",
    "Tensorflow\n",
    "\n",
    "Open-source library for graph-based numerical computation (developed by the google brain team)\n",
    "Low and high level API (use for Addition, multiplication, differentiation)\n",
    "Use and to train Machine Learning models\n",
    "In v2.0\n",
    "Eager execution by default\n",
    "Model building with Keras and Estimators\n",
    "\n",
    "Tensor\n",
    "Generalization of vectors and matrices\n",
    "Tensor is a Collection of numbers of Specific shape\n",
    "\n",
    "\n",
    "##### Defining data as constants\n",
    "Throughout this course, we will use tensorflow version 2.1 and will exclusively import the submodules needed to complete each exercise.\n",
    "\n",
    "you will use it to transform a numpy array, credit_numpy, into a tensorflow constant, credit_constant. This array contains feature columns from a dataset on credit card holders.\n",
    "\n",
    "Note that tensorflow version 2.0 allows you to use data as either a numpy array or a tensorflow constant object. Using a constant will ensure that any operations performed with that object are done in tensorflow.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"ones_11:0\", shape=(2, 2, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "#tf.enable_eager_execution()\n",
    "\n",
    "# 0D tensor\n",
    "d0=tf.ones((1,))\n",
    "\n",
    "#1D tensor\n",
    "d1=tf.ones((2,))\n",
    "\n",
    "#2D tensor\n",
    "d2=tf.ones((2,2))\n",
    "\n",
    "#3D tensor\n",
    "d3=tf.ones((2,2,2))\n",
    "\n",
    "# to print 3D tensor\n",
    "print(d3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Defining constants in TensorFlow\n",
    "A constant is the simplest category of tensor (can not be change and can not be trained), can have any dimension,\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'shape' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-bc94b7851ee7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mconstant\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m#define a 2x3 constant\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconstant\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;31m#define a 2x2 constant\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconstant\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'shape' is not defined"
     ]
    }
   ],
   "source": [
    "#define constant\n",
    "from tensorflow import constant\n",
    "#define a 2x3 constant\n",
    "a = constant(3, shape[2,3])\n",
    "#define a 2x2 constant\n",
    "b = constant([1,2,3,4], shape[2,2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using convenience functions to define constants\n",
    "tf.constant()  constant([1,2,3])\n",
    "tf.zeros() zeros([2,3])\n",
    "tf.zeros_like()  zeros_like(input_tensor)\n",
    "tf.ones()  ones([2,2])\n",
    "tf.fill()  fill([3,3],7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "credit = pd.read_csv('./dataset/credit.csv')\n",
    "credit_numpy = credit.to_numpy()\n",
    "credit.head()\n",
    "\n",
    "\n",
    "# Convert the credit_numpy array into  a tensorflow constant\n",
    "credit_constant = tf.constant(credit_numpy)\n",
    "\n",
    "# Print constant datatype\n",
    "print('The datatype is:', credit_constant.dtype)\n",
    "\n",
    "# Print constant shape\n",
    "print('The shape is:', credit_constant.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Defining variables\n",
    "Unlike a constant, a variable's value can be modified. This will be quite useful when we want to train a model by updating its parameters. Constants can't be used for this purpose, so variables are the natural choice.\n",
    "\n",
    "Let's try defining and working with a variable.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Variable:0' shape=(4,) dtype=int32, numpy=array([1, 2, 3, 4])>\n",
      "[1 2 3 4]\n"
     ]
    }
   ],
   "source": [
    "# Detine the 1-dimensional variable A1\n",
    "A1 = tf.Variable([1, 2, 3, 4])\n",
    "\n",
    "# Print the variable A1\n",
    "print(A1)\n",
    "\n",
    "# Convert A1 to a numpy array and assign it to B1\n",
    "B1 = A1.numpy()\n",
    "\n",
    "# Print B1\n",
    "print(B1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "#define a variable\n",
    "a0 = tf.Variable([1,2,3,4,5,6], dtype=tf.float32)\n",
    "a1 = tf.Variable([1,2,3,4,5,6], dtype=tf.int16)\n",
    "\n",
    "#define a constant\n",
    "b=tf.constant(2, tf.float32)\n",
    "\n",
    "#compute their product\n",
    "c= tf.multiply(a0, b)\n",
    "c1=a0*b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining data as constants\n",
    "Throughout this course, we will use tensorflow version 2.1 and will exclusively import the submodules needed to complete each exercise. This will usually be done for you, but you will do it in this exercise by importing constant from tensorflow.\n",
    "\n",
    "After you have imported constant, you will use it to transform a numpy array, credit_numpy, into a tensorflow constant, credit_constant. This array contains feature columns from a dataset on credit card holders and is previewed in the image below. We will return to this dataset in later chapters.\n",
    "\n",
    "Note that tensorflow version 2.0 allows you to use data as either a numpy array or a tensorflow constant object. Using a constant will ensure that any operations performed with that object are done in tensorflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'credit_numpy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-8d6232182533>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# Convert the credit_numpy array into a tensorflow constant\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mcredit_constant\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconstant\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcredit_numpy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;31m# Print constant datatype\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'credit_numpy' is not defined"
     ]
    }
   ],
   "source": [
    "#Import the constant submodule from the tensorflow module.\n",
    "#Convert the credit_numpy array into a constant object in tensorflow. Do not set the data type.\n",
    "# Import constant from TensorFlow\n",
    "from tensorflow import constant\n",
    "\n",
    "# Convert the credit_numpy array into a tensorflow constant\n",
    "credit_constant = constant(credit_numpy)\n",
    "\n",
    "# Print constant datatype\n",
    "print('The datatype is:', credit_constant.dtype)\n",
    "\n",
    "# Print constant shape\n",
    "print('The shape is:', credit_constant.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining variables\n",
    "Unlike a constant, a variable's value can be modified. This will be quite useful when we want to train a model by updating its parameters. Constants can't be used for this purpose, so variables are the natural choice.\n",
    "\n",
    "Let's try defining and working with a variable. Note that Variable(), which is used to create a variable tensor, has been imported from tensorflow and is available to use in the exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a variable, A1, as the 1-dimensional tensor: [1, 2, 3, 4].\n",
    "#Print A1. Do not use the .numpy() method. What did this tell you?\n",
    "#Apply .numpy() to A1 and assign it to B1.\n",
    "#Print B1. What did this tell you?\n",
    "\n",
    "import tensorflow as tf\n",
    "# Define the 1-dimensional variable A1\n",
    "A1 = tf.Variable([1, 2, 3, 4])\n",
    "\n",
    "# Print the variable A1\n",
    "print(A1)\n",
    "\n",
    "# Convert A1 to a numpy array and assign it to B1\n",
    "B1 = A1.numpy()\n",
    "\n",
    "# Print B1\n",
    "print(B1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<tf.Variable 'Variable:0' shape=(4,) dtype=int32, numpy=array([1, 2, 3, 4], dtype=int32)>\n",
    "[1 2 3 4]\n",
    "\n",
    "\n",
    "#### Basic operations\n",
    "\n",
    "Basic operations\n",
    "TensorFlow operation\n",
    "model of computation that revolves around the use of graphs\n",
    "Graph contains edges and nodes, where edges are tensors and the nodes are operations\n",
    "Performing tensor addition\n",
    "The add() operation performs element-wise addition with two tensors\n",
    "Element-wise addition requires both tensors to have the same shape:\n",
    "Scalar addition: \n",
    "\n",
    "Matrix addition: \n",
    "\n",
    "Operator is overloaded with \n",
    "+\n",
    "+\n",
    "Performing tensor multiplication\n",
    "Element-wise multiplication performed using multiply() operation\n",
    "Matrix multiplication performed with matmul() operator\n",
    "The matmul(A, B) operation multiplies A by B\n",
    "Number of columns of A must equal the number of rows of B\n",
    "Summing over tensor dimensions\n",
    "The reduce_sum() operator sums over the dimensions of a tensor\n",
    "reduce_sum(A) sums over all dimensions of A\n",
    "reduce_sum(A, i) sums over dimension i\n",
    "Performing element-wise multiplication\n",
    "Element-wise multiplication in TensorFlow is performed using two tensors with identical shapes. This is because the operation multiplies elements in corresponding positions in the two tensors. An example of an element-wise multiplication, denoted by the \n",
    "⊙\n",
    "\n",
    "In this exercise, you will perform element-wise multiplication, paying careful attention to the shape of the tensors you multiply.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import constant and add from tensorflow\n",
    "from tensorflow import constant, add\n",
    "\n",
    "#define 0-dimensional tensors\n",
    "A0=constant([1])\n",
    "B0=constant([2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define 1 dimensional tensors\n",
    "A1 = constant([1,2])\n",
    "B1 = constant([3,4])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define 2-dimensional tensors\n",
    "A2 = constant([[1,2], [3,4]])\n",
    "B2 = constant([[5,6], [7,8]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#perform tensor addition with add()\n",
    "C0=add(A0,B0)\n",
    "c1=add(A1,B1)\n",
    "C2=add(A2,B2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The add() operation performs element-wise addition with two tensors\n",
    "Element-wise addition requires both tensors to have the same shape:\n",
    "scalar addition: 1+2=3\n",
    "vector addition: [1,2] + [3,4] = [4,6]\n",
    "matrix addition: [1 2]  +  [5 6] = [6   8]\n",
    "                 [3 4]     [7 8]   [10 12]\n",
    "                 \n",
    "the add() operator is overloaded (we can do computations in claster mode)\n",
    "\n",
    "\n",
    "###### Multiplication in TensorFlow\n",
    "Element-wise multiplication performed using multiply() operation\n",
    "The tensors multiplied must have the same shape\n",
    "E.g. [1,2,3] and [3,4,5] or [1,2] and [3,4]\n",
    "Matrix multiplication performed with matmul() operator\n",
    "matmul(A,B) operation multiplies A by B\n",
    "number of columns of A must equal the number of rows of B\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import operators from tensorflow\n",
    "from tensorflow import ones, matmul, multiply\n",
    "\n",
    "#define tensors\n",
    "A0=ones(1)\n",
    "A31=ones([3,1])\n",
    "A34=ones([3,4])\n",
    "A43=ones([4,3])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What operations are valid?\n",
    "multiply(A0,A0), multiply(A31,A31) and multiply(A34,A34)\n",
    "\n",
    "matmul(A43, A34) but not matmul(A43, A43)\n",
    "\n",
    "##### Summing over tensor dimensions\n",
    "The reduce_sum() operator sums over the dimensions of a tensor\n",
    "reduce_sum(A) sums over all dimensions of A\n",
    "\n",
    "reduce_sum(A,i) sums over dimension i\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import operations from tensorflow\n",
    "from tensorflow import ones, reduce_sum\n",
    "\n",
    "#define a 2x3x4 tensor of ones\n",
    "A=ones([2,3,4])\n",
    "\n",
    "#sum over all dimensions\n",
    "B=reduce_sum(A)\n",
    "\n",
    "#sum over dimensions 0,1 and 2\n",
    "B0=reduce_sum(A,0)\n",
    "B1=reduce_sum(A,1)\n",
    "B2=reduce_sum(A,2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#practice\n",
    "# Define tensors A1 and A23 as constants\n",
    "A1 = constant([1, 2, 3, 4])\n",
    "A23 = constant([[1, 2, 3], [1, 6, 4]])\n",
    "\n",
    "# Define B1 and B23 to have the correct shape\n",
    "B1 = ones_like(A1)\n",
    "B23 = ones_like(A23)\n",
    "\n",
    "# Perform element-wise multiplication\n",
    "C1 = multiply(A1,B1)\n",
    "C23 = multiply(A23,B23)\n",
    "\n",
    "# Print the tensors C1 and C23\n",
    "print('C1: {}'.format(C1.numpy()))\n",
    "print('C23: {}'.format(C23.numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C1: [1 2 3 4]\n",
    "C23: [[1 2 3]\n",
    " [1 6 4]]\n",
    " \n",
    " \n",
    "######  Making predictions with matrix multiplication\n",
    "In later chapters, you will learn to train linear regression models. This process will yield a vector of parameters that can be multiplied by the input data to generate predictions. In this exercise, you will use input data, features, and a target vector, bill, which are taken from a credit card dataset we will use later in the course.\n",
    "\n",
    "features=⎡⎣⎢⎢⎢2,2,2,1,24,26,57,37⎤⎦⎥⎥⎥, bill=⎡⎣⎢⎢⎢3913,2682,8617,64400⎤⎦⎥⎥⎥, params=[1000,150]\n",
    "The matrix of input data, features, contains two columns: education level and age. The target vector, bill, is the size of the credit card borrower's bill.\n",
    "\n",
    "Since we have not trained the model, you will enter a guess for the values of the parameter vector, params. You will then use matmul() to perform matrix multiplication of features by params to generate predictions, billpred, which you will compare with bill. Note that we have imported matmul() and constant().\n",
    "\n",
    "\n",
    "Define features, params, and bill as constants.\n",
    "Compute the predicted value vector, billpred, by multiplying the input data, features, by the parameters, params. Use matrix multiplication, rather than the element-wise product.\n",
    "Define error as the targets, bill, minus the predicted values, billpred.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1687]\n",
      " [-3218]\n",
      " [-1933]\n",
      " [57850]]\n"
     ]
    }
   ],
   "source": [
    "#import operators from tensorflow\n",
    "from tensorflow import matmul, constant\n",
    "# Define features, params, and bill as constants\n",
    "features = constant([[2, 24], [2, 26], [2, 57], [1, 37]])\n",
    "params = constant([[1000], [150]])\n",
    "bill = constant([[3913], [2682], [8617], [64400]])\n",
    "\n",
    "# Compute billpred using features and params\n",
    "billpred = matmul(features, params)\n",
    "\n",
    "# Compute and print the error\n",
    "error = bill - billpred\n",
    "print(error.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[-1687]\n",
    " [-3218]\n",
    " [-1933]\n",
    " [57850]]\n",
    "Nice job! Understanding matrix multiplication will make things simpler when we start making predictions with linear models.\n",
    "\n",
    "Summing over tensor dimensions\n",
    "You've been given a matrix, wealth. This contains the value of bond and stock wealth for five individuals in thousands of dollars.\n",
    "         [11, 50]\n",
    "         [7, 2]\n",
    "wealth = [4, 60]\n",
    "         [3, 0]\n",
    "         [25,10]\n",
    "\n",
    "The first column corresponds to bonds and the second corresponds to stocks. Each row gives the bond and stock wealth for a single individual.\n",
    "\n",
    "Use reduce_sum() over the rows to compute total wealth for each individual.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C1: 172\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import reduce_sum \n",
    "wealth = constant([[11, 50], [7, 2], [4, 60], [3, 0], [25, 10]])\n",
    "C1 =  reduce_sum(wealth)\n",
    "print('C1: {}'.format(C1.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C2: [61  9 64  3 35]\n"
     ]
    }
   ],
   "source": [
    "#reduce_sum(wealth,1) will sum wealth over its rows.\n",
    "C2 = reduce_sum(wealth, 1)\n",
    "print('C2: {}'.format(C2.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C3: [ 50 122]\n"
     ]
    }
   ],
   "source": [
    "#reduce_sum(wealth,0) will sum wealth over its columns.\n",
    "C3 = reduce_sum(wealth, 0)\n",
    "print('C3: {}'.format(C3.numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Advances operations  gradient(), reshape(), random()\n",
    "\n",
    "gradient() - computes the slope of a function at a point\n",
    "reshape() - reshapes a tensor (e.g. 10x10 to 100x1)\n",
    "random() - populates tensor with entries drawn from a probability distribution (generates a tensor)\n",
    "\n",
    "###### Finding the optimum (that is minimum or maximum)\n",
    "In many problems, we will want to find the optimum of a function\n",
    "Minimum: lowest value of a loss function\n",
    "Maximum: highest value of objestive function\n",
    "\n",
    "We can do this using the gradient() operation\n",
    "Optimum: find a point where gradient=0\n",
    "Minimum: change in gradient >0\n",
    "Maximum change in gradient <0\n",
    "\n",
    "\n",
    "Gradients in TensorFlow\n",
    "\n",
    "\n",
    "https://campus.datacamp.com/courses/introduction-to-tensorflow-in-python/introduction-to-tensorflow?ex=8\n",
    "watch! and make notes here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import tensorflow under the alias tf\n",
    "import tensorflow as tf\n",
    " #define x \n",
    "x = tf.Variable(-1.0)\n",
    "\n",
    "\n",
    "#define y within instance of GradientTape\n",
    "#so we will calculate rate of change of x in relation to y\n",
    "with tf.GradientTape() as tape:\n",
    "    tape.watch(x)\n",
    "    y=tf.multiply(x,x)\n",
    "    \n",
    "# evaluate the gradient of y at x=-1\n",
    "g = tape.gradient(y,x)\n",
    "print(g.numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### How to reshape a grayscale image\n",
    "\n",
    "#import tensorflow as alias tf\n",
    "import tensorflow as tf\n",
    "\n",
    "#generate grayscale image\n",
    "gray = tf. random.uniform([2,2], maxval=255, dtype='int32')\n",
    "\n",
    "#reshape grayscale image\n",
    "gray = tf.reshape(gray, [2*2, 1])\n",
    "\n",
    "\n",
    "#### color image #############\n",
    "#generate color image\n",
    "color = tf. random.uniform([2,2,3], maxval=255, dtype='int32')\n",
    "\n",
    "#reshape grayscale image\n",
    "color = tf.reshape(color, [2*2, 3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reshaping tensors\n",
    "Later in the course, you will classify images of sign language letters using a neural network. In some cases, the network will take 1-dimensional tensors as inputs, but your data will come in the form of images, which will either be either 2- or 3-dimensional tensors, depending on whether they are grayscale or color images.\n",
    "\n",
    "The figure below shows grayscale and color images of the sign language letter A. The two images have been imported for you and converted to the numpy arrays gray_tensor and color_tensor. Reshape these arrays into 1-dimensional vectors using the reshape operation, which has been imported for you from tensorflow. Note that the shape of gray_tensor is 28x28 and the shape of color_tensor is 28x28x3.\n",
    "\n",
    "Reshape gray_tensor from a 28x28 matrix into a 784x1 vector named gray_vector.\n",
    "Reshape color_tensor from a 28x28x3 tensor into a 2352x1 vector named color_vector.\n",
    "\n",
    "Excellent work! Notice that there are 3 times as many elements in color_vector as there are in gray_vector, since color_tensor has 3 color channels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gray_tensor = pd.read_csv('gray_tensor.csv').to_numpy()\n",
    "\n",
    "color_tensor = pd.read_csv('color_tensor.csv').to_numpy().reshape(28, 28, 3)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow(gray_tensor, cmap=plt.cm.binary);\n",
    "# plt.imshow(image, interpolation='nearest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape the grayscale image tensor into a vector\n",
    "gray_vector = reshape(gray_tensor, (784, 1))\n",
    "\n",
    "# Reshape the color image tensor into a vector\n",
    "color_vector = reshape(color_tensor, (2352, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Optimizing with gradients\n",
    "You are given a loss function, y=x^2, which you want to minimize. You can do this by computing the slope using the GradientTape() operation at different values of x. If the slope is positive, you can decrease the loss by lowering x. If it is negative, you can decrease it by increasing x. This is how gradient descent works.\n",
    "\n",
    "The image shows a plot of y equals x squared. It also shows the gradient at x equals -1, x equals 0, and x equals 1.\n",
    "\n",
    "In practice, you will use a high level tensorflow operation to perform gradient descent automatically. In this exercise, however, you will compute the slope at x values of -1, 1, and 0. The following operations are available: GradientTape(), multiply(), and Variable().\n",
    "\n",
    "Define x as a variable with the initial value x0.\n",
    "Set the loss function, y, equal to x multiplied by x. Do not make use of operator overloading.\n",
    "Set the function to return the gradient of y with respect to x."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-2.0\n",
      "2.0\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "def compute_gradient(x0):\n",
    "    # Define x as a variable with an initial value of x0\n",
    "    x = tf.Variable(x0)\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(x)\n",
    "        # Define y using the multiply operation\n",
    "        y = x * x\n",
    "    # Return the gradient of y with respect to x\n",
    "    return tape.gradient(y, x).numpy()\n",
    "\n",
    "# Compute and print gradients at x = -1, 1, and 0\n",
    "print(compute_gradient(-1.0))\n",
    "print(compute_gradient(1.0))\n",
    "print(compute_gradient(0.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Notice that the slope is positive at x = 1, which means that we can lower the loss by reducing x. The slope is negative at x = -1, which means that we can lower the loss by increasing x. The slope at x = 0 is 0, which means that we cannot lower the loss by either increasing or decreasing x. This is because the loss is minimized at x = 0.\n",
    " \n",
    "#####  Working with image data\n",
    "You are given a black-and-white image of a letter, which has been encoded as a tensor, letter. You want to determine whether the letter is an X or a K. You don't have a trained neural network, but you do have a simple model, model, which can be used to classify letter.\n",
    "\n",
    "The 3x3 tensor, letter, and the 1x3 tensor, model, are available in the Python shell. You can determine whether letter is a K by multiplying letter by model, summing over the result, and then checking if it is equal to 1. As with more complicated models, such as neural networks, model is a collection of weights, arranged in a tensor.\n",
    "\n",
    "Note that the functions reshape(), matmul(), and reduce_sum() have been imported from tensorflow and are available for use.\n",
    "\n",
    "The model, model, is 1x3 tensor, but should be a 3x1. Reshape model.\n",
    "Perform a matrix multiplication of the 3x3 tensor, letter, by the 3x1 tensor, model.\n",
    "Sum over the resulting tensor, output, and assign this value to prediction.\n",
    "Print prediction using the .numpy() method to determine whether letter is K."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "letter = np.array([[1.0, 0, 1.0], [1., 1., 0], [1., 0, 1.] ])\n",
    "model = np.array([[1., 0., -1.]])\n",
    "\n",
    "# Reshape model from a 1x3 to a 3x1 tensor\n",
    "model = tf.reshape(model, (3, 1))\n",
    "\n",
    "# Multiply letter by model (multiply 2 matrixes)\n",
    "output = tf.matmul(letter, model)\n",
    "\n",
    "# Sum over output and print prediction using the numpy method\n",
    "prediction = tf.reduce_sum(output)\n",
    "print(prediction.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Excellent work! Your model found that prediction=1.0 and correctly classified the letter as a K. In the coming chapters, you will use data to train a model, model, and then combine this with matrix multiplication, matmul(letter, model), as we have done here, to make predictions about the classes of objects.\n",
    "\n",
    "## 02 - Linear Models\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.2.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input data\n",
    "Load data using pandas\n",
    "Before you can train a machine learning model, you must first import data. There are several valid ways to do this, but for now, we will use a simple one-liner from pandas: pd.read_csv(). Recall from the video that the first argument specifies the path or URL. All other arguments are optional.\n",
    "\n",
    "In this exercise, you will import the King County housing dataset, which we will use to train a linear model later in the chapter.\n",
    "\n",
    "Online property companies offer valuations of houses using machine learning techniques. The aim of this report is to predict the house sales in King County, Washington State\n",
    "https://www.kaggle.com/shivachandel/kc-house-data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        221900.0\n",
      "1        538000.0\n",
      "2        180000.0\n",
      "3        604000.0\n",
      "4        510000.0\n",
      "           ...   \n",
      "21608    360000.0\n",
      "21609    400000.0\n",
      "21610    402101.0\n",
      "21611    400000.0\n",
      "21612    325000.0\n",
      "Name: price, Length: 21613, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset as a dataframe named housing\n",
    "housing = pd.read_csv('kc_house_data.csv')\n",
    "\n",
    "# Print the price column of housing\n",
    "print(housing['price'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>price</th>\n",
       "      <th>bedrooms</th>\n",
       "      <th>bathrooms</th>\n",
       "      <th>sqft_living</th>\n",
       "      <th>sqft_lot</th>\n",
       "      <th>floors</th>\n",
       "      <th>waterfront</th>\n",
       "      <th>view</th>\n",
       "      <th>...</th>\n",
       "      <th>grade</th>\n",
       "      <th>sqft_above</th>\n",
       "      <th>sqft_basement</th>\n",
       "      <th>yr_built</th>\n",
       "      <th>yr_renovated</th>\n",
       "      <th>zipcode</th>\n",
       "      <th>lat</th>\n",
       "      <th>long</th>\n",
       "      <th>sqft_living15</th>\n",
       "      <th>sqft_lot15</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7129300520</td>\n",
       "      <td>20141013T000000</td>\n",
       "      <td>221900.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1180</td>\n",
       "      <td>5650</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>1180</td>\n",
       "      <td>0</td>\n",
       "      <td>1955</td>\n",
       "      <td>0</td>\n",
       "      <td>98178</td>\n",
       "      <td>47.5112</td>\n",
       "      <td>-122.257</td>\n",
       "      <td>1340</td>\n",
       "      <td>5650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6414100192</td>\n",
       "      <td>20141209T000000</td>\n",
       "      <td>538000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.25</td>\n",
       "      <td>2570</td>\n",
       "      <td>7242</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>2170</td>\n",
       "      <td>400</td>\n",
       "      <td>1951</td>\n",
       "      <td>1991</td>\n",
       "      <td>98125</td>\n",
       "      <td>47.7210</td>\n",
       "      <td>-122.319</td>\n",
       "      <td>1690</td>\n",
       "      <td>7639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5631500400</td>\n",
       "      <td>20150225T000000</td>\n",
       "      <td>180000.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.00</td>\n",
       "      <td>770</td>\n",
       "      <td>10000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>6</td>\n",
       "      <td>770</td>\n",
       "      <td>0</td>\n",
       "      <td>1933</td>\n",
       "      <td>0</td>\n",
       "      <td>98028</td>\n",
       "      <td>47.7379</td>\n",
       "      <td>-122.233</td>\n",
       "      <td>2720</td>\n",
       "      <td>8062</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           id             date     price  bedrooms  bathrooms  sqft_living  \\\n",
       "0  7129300520  20141013T000000  221900.0         3       1.00         1180   \n",
       "1  6414100192  20141209T000000  538000.0         3       2.25         2570   \n",
       "2  5631500400  20150225T000000  180000.0         2       1.00          770   \n",
       "\n",
       "   sqft_lot  floors  waterfront  view  ...  grade  sqft_above  sqft_basement  \\\n",
       "0      5650     1.0           0     0  ...      7        1180              0   \n",
       "1      7242     2.0           0     0  ...      7        2170            400   \n",
       "2     10000     1.0           0     0  ...      6         770              0   \n",
       "\n",
       "   yr_built  yr_renovated  zipcode      lat     long  sqft_living15  \\\n",
       "0      1955             0    98178  47.5112 -122.257           1340   \n",
       "1      1951          1991    98125  47.7210 -122.319           1690   \n",
       "2      1933             0    98028  47.7379 -122.233           2720   \n",
       "\n",
       "   sqft_lot15  \n",
       "0        5650  \n",
       "1        7639  \n",
       "2        8062  \n",
       "\n",
       "[3 rows x 21 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "housing.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that you did not have to specify a delimiter with the sep parameter, since the dataset was stored in the default, comma-separated format.\n",
    "\n",
    "#### Setting the data type\n",
    "In this exercise, you will both load data and set its type. You will import numpy and tensorflow, and define tensors that are usable in tensorflow using columns in housing with a given data type. Recall that you can select the price column, for instance, from housing using housing['price']."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[221900. 538000. 180000. ... 402101. 400000. 325000.]\n",
      "tf.Tensor([False False False ... False False False], shape=(21613,), dtype=bool)\n"
     ]
    }
   ],
   "source": [
    "# Use a numpy array to define price as a 32-bit float\n",
    "price = np.array(housing['price'], np.float32)\n",
    "\n",
    "# Define waterfront as a Boolean using case\n",
    "waterfront = tf.cast(housing['waterfront'], tf.bool)\n",
    "\n",
    "# Print price and waterfront\n",
    "print(price)\n",
    "print(waterfront)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that printing price yielded a numpy array; whereas printing waterfront yielded a tf.Tensor().\n",
    "\n",
    "#### Loss functions  (errors) difference between predictions and true targets\n",
    "https://towardsdatascience.com/importance-of-loss-function-in-machine-learning-eddaaec69519\n",
    "\n",
    "Loss function\n",
    "Fundamental tensorflow operation\n",
    "Used to train model (smaller loss function -> better)\n",
    "Measure a model fit\n",
    "Higher value -> worse fit\n",
    "Minimize the loss function (adjust tge algorithms)\n",
    "\n",
    "\n",
    "###### Common loss functions in Tensorflow\n",
    "Mean squared error (MSE)\n",
    "Mean absolute error (MAE)\n",
    "Huber error\n",
    "Hinge error\n",
    "Log-Likelyhood error\n",
    "\n",
    "###### Regression Losses\n",
    "Mean Square Error/Quadratic Loss/L2 Loss\n",
    "\n",
    "###### Classification Losses\n",
    "Hinge Loss/Multi class SVM Loss\n",
    "\n",
    "<br>\n",
    "###### Mean Squared Error (MSE)\n",
    "MSE is the workspace of basic loss functions, as it is easy to understand and implement and generally works pretty well. To calculate MSE, you take the difference between your model’s predictions and the ground truth, square it out and then average it out across the whole dataset.\n",
    "The result is always positive regardless of the sign of the predicted and ground truth values and a perfect value is 0.0.\n",
    "<br>\n",
    "###### function to calculate MSE\n",
    "def MSE(y_predicted, y_actual):\n",
    "    squared_error = (y_predicted - y_actual) ** 2\n",
    "    sum_squared_error = np.sum(squared_error)\n",
    "    mse = sum_squared_error / y_actual.size\n",
    "    return mse\n",
    "\n",
    "<br>\n",
    "\n",
    "######  Mean Absolute Error (MAE)\n",
    "Mean Absolute Error (MAE) is only slightly different in definition from the MSE, but interestingly provides almost exactly opposite properties. To calculate the MAE, you take the difference between your model’s predictions and the ground truth, apply the absolute value to that difference, and then average it out across the whole dataset.\n",
    "\n",
    "###### function to calculate MAE\n",
    "def MAE(y_predicted, y_actual):\n",
    "    \n",
    "    abs_error = np.abs(y_predicted - y_actual)\n",
    "    sum_abs_error = np.sum(abs_error)\n",
    "    mae = sum_abs_error / y_actual.size\n",
    "    \n",
    "    return mae\n",
    "\n",
    "######  Log-Likelihood Loss\n",
    "This loss function is also relatively simple and commonly used in classification problems. In this,\n",
    "the error between two probability distributions is measured using cross-entropy.\n",
    "-(y_actual * log(y_predicted) + (1 - y_actual) * log(1 - y_predicted))\n",
    "\n",
    "Cross-entropy for a binary or two-class prediction problem is actually calculated as the average cross-entropy across all examples.\n",
    "from math import log\n",
    " \n",
    "###### function to calculate binary cross entropy\n",
    "def binary_cross_entropy(actual, predicted):\n",
    "\tsum_score = 0.0\n",
    "\tfor i in range(len(actual)):\n",
    "\t\tsum_score += actual[i] * log(1e-15 + predicted[i])\n",
    "\tmean_sum_score = 1.0 / len(actual) * sum_score\n",
    "\treturn -mean_sum_score\n",
    "\n",
    "This function is one of the most popular measures for Kaggle competitions. It’s just a straightforward modification of the likelihood function with logarithms.\n",
    "\n",
    "\n",
    "###### Hinge Loss\n",
    "The Hinge loss function is popular with Support Vector Machines(SVMs). These are used for training the classifiers. Let ‘t’ be the target output such that t = -1 or 1, and the classifier score be ‘y’, then the hinge loss for the prediction is given as: L(y) = max(0, 1-t.y)\n",
    "\n",
    "###### Huber Loss\n",
    "We know that MSE is great for learning outliers while the MAE is great for ignoring them. But what about something in the middle?\n",
    "Consider an example where we have a dataset of 100 values we would like our model to be trained to predict. Out of all that data, 25% of the expected values are 5 while the other 75% is 10.\n",
    "An MSE loss wouldn’t quite do the trick since we don’t have “outliers”; 25% is by no means a small fraction. On the other hand, we don’t necessarily want to weight that 25% too low with an MAE. Those values of 5 aren’t close to the median (10 — since 75% of the points have a value of 10), but they’re also not outliers.\n",
    "\n",
    "The Huber Loss offers the best of both worlds by balancing the MSE and MAE together. \n",
    "###### function to calculate Huber loss\n",
    "def huber_loss(y_predicted, y_actual, delta=1.0):\n",
    "    huber_mse = 0.5*(y_actual-y_predicted)**2\n",
    "    huber_mae = delta * (np.abs(y_actual - y_predicted) - 0.5 * delta)\n",
    "    \n",
    "    return np.where(np.abs(y_actual - y_predicted) <= delta, \n",
    "                             huber_mse,  huber_mae)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "###### Why do we care about loss functions?\n",
    "MSE\n",
    "Strongly penalizes outliers\n",
    "High (gradient) sensitivity near minimum\n",
    "MAE\n",
    "Scales linearly with size of error\n",
    "Low sensitivity near minimum\n",
    "Huber\n",
    "Similar to MSE near minimum\n",
    "Similar to MAE away from minimum\n",
    "\n",
    "\n",
    "##### Loss functions in TensorFlow\n",
    "In this exercise, you will compute the loss using data from the King County housing dataset. You are given a target, price, which is a tensor of house prices, and predictions, which is a tensor of predicted house prices. You will evaluate the loss function and print out the value of the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kc_sample = pd.read_csv('./dataset/loss_price.csv')\n",
    "price = kc_sample['price'].to_numpy()\n",
    "predictions = kc_sample['pred'].to_numpy()\n",
    "\n",
    "# Compute the mean squared error (mse)\n",
    "loss = tf.keras.losses.mse(price, predictions)\n",
    "\n",
    "# Print the mean squared error (mse)\n",
    "print(loss.numpy())\n",
    "#141171604777.12717\n",
    "\n",
    "# Compute the mean squared error (mse)\n",
    "loss = tf.keras.losses.mae(price, predictions)\n",
    "\n",
    "# Print the mean squared error (mse)\n",
    "print(loss.numpy())\n",
    "\n",
    "#268827.9930208799"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "You may have noticed that the MAE was much smaller than the MSE, even though price and predictions were the same. This is because the different loss functions penalize deviations of predictions from price differently. MSE does not like large deviations and punishes them harshly.\n",
    "\n",
    "##### Modifying the loss function\n",
    "In the previous exercise, you defined a tensorflow loss function and then evaluated it once for a set of actual and predicted values. In this exercise, you will compute the loss within another function called loss_function(), which first generates predicted values from the data and variables. The purpose of this is to construct a function of the trainable model variables that returns the loss. You can then repeatedly evaluate this function for different variable values until you find the minimum. In practice, you will pass this function to an optimizer in tensorflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np \n",
    "\n",
    "features = tf.constant([1, 2, 3, 4, 5], dtype=tf.float32)\n",
    "targets = tf.constant([2, 4, 6, 8, 10], dtype=tf.float32)\n",
    "\n",
    "\n",
    "# Initialize a variable named scalar\n",
    "scalar = tf.Variable(1.0, tf.float32)\n",
    "\n",
    "# Define the model\n",
    "def model(scalar, features=features):\n",
    "    return scalar * features\n",
    "\n",
    "# Define a loss function\n",
    "def loss_function(scalar, features=features, targets=targets):\n",
    "    # Compute the predicted values\n",
    "    predictions = model(scalar, features)\n",
    "    \n",
    "    # Return the mean absolute error loss\n",
    "    return tf.keras.losses.mae(targets, predictions)\n",
    "\n",
    "# Evaluate the loss function and print the loss\n",
    "print(loss_function(scalar).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Linear regression\n",
    "Set up a linear regression\n",
    "A univariate linear regression identifies the relationship between a single feature and the target tensor. In this exercise, we will use a property's lot size and price. Just as we discussed in the video, we will take the natural logarithms of both tensors, which are available as price_log and size_log.\n",
    "\n",
    "In this exercise, you will define the model and the loss function. You will then evaluate the loss function for two different values of intercept and slope. Remember that the predicted values are given by intercept + features * slope.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "size_log = np.log(np.array(housing['sqft_lot'], np.float32))\n",
    "price_log = np.log(np.array(housing['price'], np.float32))\n",
    "bedrooms = np.array(housing['bedrooms'], np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "145.44652\n",
      "71.866\n"
     ]
    }
   ],
   "source": [
    "# Define a linear regression model\n",
    "def linear_regression(intercept, slope, features=size_log):\n",
    "    return intercept + slope * features\n",
    "\n",
    "# Set loss_function() to take the variables as arguments\n",
    "def loss_function(intercept, slope, features=size_log, targets=price_log):\n",
    "    # Set the predicted values\n",
    "    predictions = linear_regression(intercept, slope, features)\n",
    "    \n",
    "    # Return the mean squared error loss\n",
    "    return tf.keras.losses.mse(targets, predictions)\n",
    "\n",
    "# Compute the loss function for different slope and intercept values\n",
    "print(loss_function(0.1, 0.1).numpy())\n",
    "print(loss_function(0.1, 0.5).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Train a linear model\n",
    "In this exercise, we will pick up where the previous exercise ended. The intercept and slope, have been defined and initialized. Additionally, a function has been defined, loss_function(intercept, slope), which computes the loss using the data and model variables.\n",
    "\n",
    "You will now define an optimization operation as opt. You will then train a univariate linear model by minimizing the loss to find the optimal values of intercept and slope. Note that the opt operation will try to move closer to the optimum with each step, but will require many steps to find it. Thus, you must repeatedly execute the operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_results(intercept, slope):\n",
    "    size_range = np.linspace(6,14,100)\n",
    "    price_pred = [intercept + slope * s for s in size_range]\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.scatter(size_log, price_log, color = 'black');\n",
    "    plt.plot(size_range, price_pred, linewidth=3.0, color='red');\n",
    "    plt.xlabel('log(size)');\n",
    "    plt.ylabel('log(price)');\n",
    "    plt.title('Scatterplot of data and fitted regression line');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65.26372\n",
      "1.4908674\n",
      "2.3815553\n",
      "2.908477\n",
      "2.6110487\n",
      "1.7605181\n",
      "1.3468053\n",
      "1.3559628\n",
      "1.2884119\n",
      "1.2425314\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "intercept = tf.Variable(0.0, tf.float32)\n",
    "slope = tf.Variable(0.0, tf.float32)\n",
    "\n",
    "# Initialize an adam optimizer\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=0.5)\n",
    "\n",
    "for j in range(100):\n",
    "    # Apply minimize, pass the loss function, and supply the variables\n",
    "    opt.minimize(lambda: loss_function(intercept, slope), var_list=[intercept, slope])\n",
    "    \n",
    "    # Print every 10th value of the loss\n",
    "    if j % 10 == 0:\n",
    "        print(loss_function(intercept, slope).numpy())\n",
    "        \n",
    "# Plot data and regressoin line\n",
    "plot_results(intercept, slope)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we printed loss_function(intercept, slope) every 10th execution for 100 executions. Each time, the loss got closer to the minimum as the optimizer moved the slope and intercept parameters closer to their optimal values.\n",
    "\n",
    "##### Multiple linear regression\n",
    "In most cases, performing a univariate linear regression will not yield a model that is useful for making accurate predictions. In this exercise, you will perform a multiple regression, which uses more than one feature.\n",
    "\n",
    "You will use price_log as your target and size_log and bedrooms as your features. Each of these tensors has been defined and is available. You will also switch from using the the mean squared error loss to the mean absolute error loss: keras.losses.mae(). Finally, the predicted values are computed as follows:params[0] + feature1 * params[1] + feature2 * params[2]. Note that we've defined a vector of parameters, params, as a variable, rather than using three variables. Here, params[0] is the intercept and params[1] and params[2] are the slopes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_results(params):\n",
    "    return print('loss: {:0.3f}, intercept: {:0.3f}, slope_1: {:0.3f}, slope_2: {:0.3f}'\n",
    "                 .format(loss_function(params).numpy(), \n",
    "                         params[0].numpy(), \n",
    "                         params[1].numpy(), \n",
    "                         params[2].numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 12.418, intercept: 0.101, slope_1: 0.051, slope_2: 0.021\n",
      "loss: 12.404, intercept: 0.102, slope_1: 0.052, slope_2: 0.022\n",
      "loss: 12.391, intercept: 0.103, slope_1: 0.053, slope_2: 0.023\n",
      "loss: 12.377, intercept: 0.104, slope_1: 0.054, slope_2: 0.024\n",
      "loss: 12.364, intercept: 0.105, slope_1: 0.055, slope_2: 0.025\n",
      "loss: 12.351, intercept: 0.106, slope_1: 0.056, slope_2: 0.026\n",
      "loss: 12.337, intercept: 0.107, slope_1: 0.057, slope_2: 0.027\n",
      "loss: 12.324, intercept: 0.108, slope_1: 0.058, slope_2: 0.028\n",
      "loss: 12.311, intercept: 0.109, slope_1: 0.059, slope_2: 0.029\n",
      "loss: 12.297, intercept: 0.110, slope_1: 0.060, slope_2: 0.030\n"
     ]
    }
   ],
   "source": [
    "params = tf.Variable([0.1, 0.05, 0.02], tf.float32)\n",
    "\n",
    "# Define the linear regression model\n",
    "def linear_regression(params, feature1=size_log, feature2=bedrooms):\n",
    "    return params[0] + feature1 * params[1] + feature2 * params[2]\n",
    "\n",
    "# Define the loss function\n",
    "def loss_function(params, targets=price_log, feature1=size_log, feature2=bedrooms):\n",
    "    # Set the predicted values\n",
    "    predictions = linear_regression(params, feature1, feature2)\n",
    "    \n",
    "    # Use the mean absolute error loss\n",
    "    return tf.keras.losses.mae(targets, predictions)\n",
    "\n",
    "# Define the optimize operation\n",
    "opt = tf.keras.optimizers.Adam()\n",
    "\n",
    "# Perform minimization and print trainable variables\n",
    "for j in range(10):\n",
    "    opt.minimize(lambda: loss_function(params), var_list=[params])\n",
    "    print_results(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that params[2] tells us how much the price will increase in percentage terms if we add one more bedroom. You could train params[2] and the other model parameters by increasing the number of times we iterate over opt.\n",
    "\n",
    "## Batch training\n",
    "\n",
    "###### Full sample versus batch training\n",
    "Full sample\n",
    "One update per epoch\n",
    "Accepts dataset without modification\n",
    "Limited by memory\n",
    "\n",
    "###### Batch Training\n",
    "Multiple updates per epoch\n",
    "Requires division of dataset\n",
    "No limit on dataset size\n",
    "\n",
    "\n",
    "###### Preparing to batch train\n",
    "Before we can train a linear model in batches, we must first define variables, a loss function, and an optimization operation. In this exercise, we will prepare to train a model that will predict price_batch, a batch of house prices, using size_batch, a batch of lot sizes in square feet. In contrast to the previous lesson, we will do this by loading batches of data using pandas, converting it to numpy arrays, and then using it to minimize the loss function in steps.\n",
    "\n",
    "Note that you should not set default argument values for either the model or loss function, since we will generate the data in batches during the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the intercept and slope\n",
    "intercept = tf.Variable(10.0, tf.float32)\n",
    "slope = tf.Variable(0.5, tf.float32)\n",
    "\n",
    "# Define the model\n",
    "def linear_regression(intercept, slope, features):\n",
    "    # Define the predicted values\n",
    "    return intercept + slope * features\n",
    "\n",
    "# Define the loss function\n",
    "def loss_function(intercept, slope, targets, features):\n",
    "    # Define the predicted values\n",
    "    predictions = linear_regression(intercept, slope, features)\n",
    "    \n",
    "    # Define the MSE loss\n",
    "    return tf.keras.losses.mse(targets, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we did not use default argument values for the input data, features and targets. This is because the input data has not been defined in advance. Instead, with batch training, we will load it during the training process.\n",
    "\n",
    "Training a linear model in batches\n",
    "In this exercise, we will train a linear regression model in batches, starting where we left off in the previous exercise. We will do this by stepping through the dataset in batches and updating the model's variables, intercept and slope, after each step. This approach will allow us to train with datasets that are otherwise too large to hold in memory.\n",
    "\n",
    "Note that the loss function,loss_function(intercept, slope, targets, features), has been defined for you. The trainable variables should be entered into var_list in the order in which they appear as loss function arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.217888 0.7016\n"
     ]
    }
   ],
   "source": [
    "intercept = tf.Variable(10.0, tf.float32)\n",
    "slope = tf.Variable(0.5, tf.float32)\n",
    "\n",
    "# Initialize adam optimizer\n",
    "opt = tf.keras.optimizers.Adam()\n",
    "\n",
    "# Load data in batches\n",
    "for batch in pd.read_csv('kc_house_data.csv', chunksize=100):\n",
    "    size_batch = np.array(batch['sqft_lot'], np.float32)\n",
    "    \n",
    "    # Extract the price values for the current batch\n",
    "    price_batch = np.array(batch['price'], np.float32)\n",
    "    \n",
    "    # Complete the loss, fill in the variable list, and minimize\n",
    "    opt.minimize(lambda: loss_function(intercept, slope, price_batch, size_batch), \n",
    "                 var_list=[intercept, slope])\n",
    "    \n",
    "# Print trained parameters\n",
    "print(intercept.numpy(), slope.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimization\n",
    "https://towardsdatascience.com/optimizers-for-training-neural-network-59450d71caf6\n",
    "\n",
    "Next we optimize our loss using an optimization algorithm. Optimization algorithms need to know how far to step in each iteration. This distance is controlled by the learning rate. If our learning rate is too big, our algorithm might overshoot the minimum and if our learning rate is too small, our algorithm may take too long to converge; this is related to the vanishing and exploding gradient problem.\n",
    "\n",
    "\n",
    "Many people may be using optimizers while training the neural network without knowing that the method is known as optimization. Optimizers are algorithms or methods used to change the attributes of your neural network such as weights and learning rate in order to reduce the losses.\n",
    "\n",
    "##### Gradient Descent\n",
    "Gradient Descent is the most basic but most used optimization algorithm. It’s used heavily in linear regression and classification algorithms. Backpropagation in neural networks also uses a gradient descent algorithm.\n",
    "Gradient descent is a first-order optimization algorithm which is dependent on the first order derivative of a loss function. It calculates that which way the weights should be altered so that the function can reach a minima. Through backpropagation, the loss is transferred from one layer to another and the model’s parameters also known as weights are modified depending on the losses so that the loss can be minimized.\n",
    "algorithm: θ=θ−α⋅∇J(θ)\n",
    "\n",
    "Advantages:\n",
    "Easy computation.\n",
    "Easy to implement.\n",
    "Easy to understand.\n",
    "Disadvantages:\n",
    "May trap at local minima.\n",
    "Weights are changed after calculating gradient on the whole dataset. So, if the dataset is too large than this may take years to converge to the minima.\n",
    "Requires large memory to calculate gradient on the whole dataset\n",
    "\n",
    "##### Adam\n",
    "Adam (Adaptive Moment Estimation) works with momentums of first and second order. The intuition behind the Adam is that we don’t want to roll so fast just because we can jump over the minimum, we want to decrease the velocity a little bit for a careful search. In addition to storing an exponentially decaying average of past squared gradients like AdaDelta, Adam also keeps an exponentially decaying average of past gradients M(t).\n",
    "\n",
    "Advantages:\n",
    "The method is too fast and converges rapidly.\n",
    "Rectifies vanishing learning rate, high variance.\n",
    "Disadvantages:\n",
    "Computationally costly.\n",
    "\n",
    "\n",
    "Conclusions\n",
    "Adam is the best optimizers. If one wants to train the neural network in less time and more efficiently than Adam is the optimizer.\n",
    "For sparse data use the optimizers with dynamic learning rate.\n",
    "If, want to use gradient descent algorithm than min-batch gradient descent is the best option.\n",
    "\n",
    "## Neural Networks\n",
    "The previous chapters taught you how to build models in TensorFlow 2.0. In this chapter, you will apply those same tools to build, train, and make predictions with neural networks. You will learn how to define dense layers, apply activation functions, select an optimizer, and apply regularization to reduce overfitting. You will take advantage of TensorFlow's flexibility by using both low-level linear algebra and high-level Keras API operations to define and train models. This is the Summary of lecture \"Introduction to TensorFlow in Python\", via datacamp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.2.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (8, 8)\n",
    "\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Dense layers\n",
    "The linear algebra of dense layers\n",
    "There are two ways to define a dense layer in tensorflow. The first involves the use of low-level, linear algebraic operations. The second makes use of high-level keras operations. In this exercise, we will use the first method to construct the network shown in the image below. <br>\n",
    "0  -   0   -   0 <br>\n",
    "   0   -    0 <br>\n",
    "       0  (all connected) <br>\n",
    "\n",
    "The input layer contains 3 features -- education, marital status, and age -- which are available as borrower_features. The hidden layer contains 2 nodes and the output layer contains a single node.\n",
    "\n",
    "\n",
    "For each layer, you will take the previous layer as an input, initialize a set of weights, compute the product of the inputs and weights, and then apply an activation function.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dense1's output shape: (1, 2)\n"
     ]
    }
   ],
   "source": [
    "borrower_features = np.array([[2., 2., 43.]], np.float32)\n",
    "# Initialize bias1\n",
    "bias1 = tf.Variable(1.0, tf.float32)\n",
    "\n",
    "# Initialize weights1 as 3x2 variable of ones\n",
    "weights1 = tf.Variable(tf.ones((3, 2)))\n",
    "\n",
    "# Perform matrix multiplication of borrower_features and weights1\n",
    "product1 = tf.matmul(borrower_features, weights1)\n",
    "\n",
    "# Apply sigmoid activation function to product1 + bias1\n",
    "dense1 = tf.keras.activations.sigmoid(product1 + bias1)\n",
    "\n",
    "# Print shape of dense1\n",
    "print(\"dense1's output shape: {}\".format(dense1.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction: 0.9525741338729858\n",
      "\n",
      " actual: 1\n"
     ]
    }
   ],
   "source": [
    "# Initialize bias2 and weights2\n",
    "bias2 = tf.Variable(1.0)\n",
    "weights2 = tf.Variable(tf.ones((2, 1)))\n",
    "\n",
    "# Perform matrix multiplication of dense1 and weights2\n",
    "product2 = tf.matmul(dense1, weights2)\n",
    "\n",
    "# Apply activation to product2 + bias2 and print the prediction\n",
    "prediction = tf.keras.activations.sigmoid(product2 + bias2)\n",
    "print('prediction: {}'.format(prediction.numpy()[0, 0]))\n",
    "print('\\n actual: 1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model produces predicted values in the interval between 0 and 1. For the example we considered, the actual value was 1 and the predicted value was a probability between 0 and 1. This, of course, is not meaningful, since we have not yet trained our model's parameters.\n",
    "\n",
    "The low-level approach with multiple examples\n",
    "In this exercise, we'll build further intuition for the low-level approach by constructing the first dense hidden layer for the case where we have multiple examples. We'll assume the model is trained and the first layer weights, weights1, and bias, bias1, are available. We'll then perform matrix multiplication of the borrower_features tensor by the weights1 variable. Recall that the borrower_features tensor includes education, marital status, and age. Finally, we'll apply the sigmoid function to the elements of products1 + bias1, yielding dense1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " shape of borrower_features:  (1, 3)\n",
      "\n",
      " shape of weights1:  (3, 2)\n",
      "\n",
      " shape of bias1:  (1,)\n",
      "\n",
      " shape of dense1:  (1, 2)\n"
     ]
    }
   ],
   "source": [
    "bias1 = tf.Variable([0.1], tf.float32)\n",
    "\n",
    "# Compute the product of borrower_features and weights1\n",
    "products1 = tf.matmul(borrower_features, weights1)\n",
    "\n",
    "# Apply a sigmoid activation function to products1 + bias1\n",
    "dense1 = tf.keras.activations.sigmoid(products1 + bias1)\n",
    "\n",
    "# Print the shapes of borrower_features, weights1, bias1, and dense1\n",
    "print('\\n shape of borrower_features: ', borrower_features.shape)\n",
    "print('\\n shape of weights1: ', weights1.shape)\n",
    "print('\\n shape of bias1: ', bias1.shape)\n",
    "print('\\n shape of dense1: ', dense1.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that our input data, borrower_features, is 5x3 because it consists of 5 examples for 3 features. The shape of weights1 is 3x2, as it was in the previous exercise, since it does not depend on the number of examples. Additionally, bias1 is a scalar. Finally, dense1 is 5x2, which means that we can multiply it by the following set of weights, weights2, which we defined to be 2x1 in the previous exercise.\n",
    "\n",
    "Using the dense layer operation\n",
    "We've now seen how to define dense layers in tensorflow using linear algebra. In this exercise, we'll skip the linear algebra and let keras work out the details. This will allow us to construct the network below, which has 2 hidden layers and 10 features, using less code than we needed for the network with 1 hidden layer and 3 features.\n",
    "To construct this network, we'll need to define three dense layers, each of which takes the previous layer as an input, multiplies it by weights, and applies an activation function.\n",
    "\n",
    "\n",
    "Dataset Information\n",
    "This dataset contains information on default payments, demographic factors, credit data, history of payment, and bill statements of credit card clients in Taiwan from April 2005 to September 2005.\n",
    "\n",
    "Content\n",
    "There are 25 variables:\n",
    "\n",
    "ID: ID of each client\n",
    "LIMIT_BAL: Amount of given credit in NT dollars (includes individual and family/supplementary credit\n",
    "SEX: Gender (1=male, 2=female)\n",
    "EDUCATION: (1=graduate school, 2=university, 3=high school, 4=others, 5=unknown, 6=unknown)\n",
    "MARRIAGE: Marital status (1=married, 2=single, 3=others)\n",
    "AGE: Age in years\n",
    "PAY_0: Repayment status in September, 2005 (-1=pay duly, 1=payment delay for one month, 2=payment delay for two months, … 8=payment delay for eight months, 9=payment delay for nine months and above)\n",
    "PAY_2: Repayment status in August, 2005 (scale same as above)\n",
    "PAY_3: Repayment status in July, 2005 (scale same as above)\n",
    "PAY_4: Repayment status in June, 2005 (scale same as above)\n",
    "PAY_5: Repayment status in May, 2005 (scale same as above)\n",
    "PAY_6: Repayment status in April, 2005 (scale same as above)\n",
    "BILL_AMT1: Amount of bill statement in September, 2005 (NT dollar)\n",
    "BILL_AMT2: Amount of bill statement in August, 2005 (NT dollar)\n",
    "BILL_AMT3: Amount of bill statement in July, 2005 (NT dollar)\n",
    "BILL_AMT4: Amount of bill statement in June, 2005 (NT dollar)\n",
    "BILL_AMT5: Amount of bill statement in May, 2005 (NT dollar)\n",
    "BILL_AMT6: Amount of bill statement in April, 2005 (NT dollar)\n",
    "PAY_AMT1: Amount of previous payment in September, 2005 (NT dollar)\n",
    "PAY_AMT2: Amount of previous payment in August, 2005 (NT dollar)\n",
    "PAY_AMT3: Amount of previous payment in July, 2005 (NT dollar)\n",
    "PAY_AMT4: Amount of previous payment in June, 2005 (NT dollar)\n",
    "PAY_AMT5: Amount of previous payment in May, 2005 (NT dollar)\n",
    "PAY_AMT6: Amount of previous payment in April, 2005 (NT dollar)\n",
    "default.payment.next.month: Default payment (1=yes, 0=no)\n",
    "Inspiration\n",
    "Some ideas for exploration:\n",
    "\n",
    "How does the probability of default payment vary by categories of different demographic variables?\n",
    "Which variables are the strongest predictors of default payment?\n",
    "Acknowledgements\n",
    "Any publications based on this dataset should acknowledge the following:\n",
    "\n",
    "Lichman, M. (2013). UCI Machine Learning Repository [http://archive.ics.uci.edu/ml]. Irvine, CA: University of California, School of Information and Computer Science.\n",
    "\n",
    "The original dataset can be found here at the UCI Machine Learning Repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>LIMIT_BAL</th>\n",
       "      <th>SEX</th>\n",
       "      <th>EDUCATION</th>\n",
       "      <th>MARRIAGE</th>\n",
       "      <th>AGE</th>\n",
       "      <th>PAY_0</th>\n",
       "      <th>PAY_2</th>\n",
       "      <th>PAY_3</th>\n",
       "      <th>PAY_4</th>\n",
       "      <th>...</th>\n",
       "      <th>BILL_AMT4</th>\n",
       "      <th>BILL_AMT5</th>\n",
       "      <th>BILL_AMT6</th>\n",
       "      <th>PAY_AMT1</th>\n",
       "      <th>PAY_AMT2</th>\n",
       "      <th>PAY_AMT3</th>\n",
       "      <th>PAY_AMT4</th>\n",
       "      <th>PAY_AMT5</th>\n",
       "      <th>PAY_AMT6</th>\n",
       "      <th>default.payment.next.month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>20000.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>689.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>120000.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>26</td>\n",
       "      <td>-1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>3272.0</td>\n",
       "      <td>3455.0</td>\n",
       "      <td>3261.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>90000.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>14331.0</td>\n",
       "      <td>14948.0</td>\n",
       "      <td>15549.0</td>\n",
       "      <td>1518.0</td>\n",
       "      <td>1500.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>50000.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>28314.0</td>\n",
       "      <td>28959.0</td>\n",
       "      <td>29547.0</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>1100.0</td>\n",
       "      <td>1069.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>50000.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>57</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>20940.0</td>\n",
       "      <td>19146.0</td>\n",
       "      <td>19131.0</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>36681.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>9000.0</td>\n",
       "      <td>689.0</td>\n",
       "      <td>679.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID  LIMIT_BAL  SEX  EDUCATION  MARRIAGE  AGE  PAY_0  PAY_2  PAY_3  PAY_4  \\\n",
       "0   1    20000.0    2          2         1   24      2      2     -1     -1   \n",
       "1   2   120000.0    2          2         2   26     -1      2      0      0   \n",
       "2   3    90000.0    2          2         2   34      0      0      0      0   \n",
       "3   4    50000.0    2          2         1   37      0      0      0      0   \n",
       "4   5    50000.0    1          2         1   57     -1      0     -1      0   \n",
       "\n",
       "   ...  BILL_AMT4  BILL_AMT5  BILL_AMT6  PAY_AMT1  PAY_AMT2  PAY_AMT3  \\\n",
       "0  ...        0.0        0.0        0.0       0.0     689.0       0.0   \n",
       "1  ...     3272.0     3455.0     3261.0       0.0    1000.0    1000.0   \n",
       "2  ...    14331.0    14948.0    15549.0    1518.0    1500.0    1000.0   \n",
       "3  ...    28314.0    28959.0    29547.0    2000.0    2019.0    1200.0   \n",
       "4  ...    20940.0    19146.0    19131.0    2000.0   36681.0   10000.0   \n",
       "\n",
       "   PAY_AMT4  PAY_AMT5  PAY_AMT6  default.payment.next.month  \n",
       "0       0.0       0.0       0.0                           1  \n",
       "1    1000.0       0.0    2000.0                           1  \n",
       "2    1000.0    1000.0    5000.0                           0  \n",
       "3    1100.0    1069.0    1000.0                           0  \n",
       "4    9000.0     689.0     679.0                           0  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('uci_credit_card.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>LIMIT_BAL</th>\n",
       "      <th>SEX</th>\n",
       "      <th>EDUCATION</th>\n",
       "      <th>MARRIAGE</th>\n",
       "      <th>AGE</th>\n",
       "      <th>PAY_0</th>\n",
       "      <th>PAY_2</th>\n",
       "      <th>PAY_3</th>\n",
       "      <th>PAY_4</th>\n",
       "      <th>...</th>\n",
       "      <th>BILL_AMT4</th>\n",
       "      <th>BILL_AMT5</th>\n",
       "      <th>BILL_AMT6</th>\n",
       "      <th>PAY_AMT1</th>\n",
       "      <th>PAY_AMT2</th>\n",
       "      <th>PAY_AMT3</th>\n",
       "      <th>PAY_AMT4</th>\n",
       "      <th>PAY_AMT5</th>\n",
       "      <th>PAY_AMT6</th>\n",
       "      <th>default.payment.next.month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>30000.000000</td>\n",
       "      <td>30000.000000</td>\n",
       "      <td>30000.000000</td>\n",
       "      <td>30000.000000</td>\n",
       "      <td>30000.000000</td>\n",
       "      <td>30000.000000</td>\n",
       "      <td>30000.000000</td>\n",
       "      <td>30000.000000</td>\n",
       "      <td>30000.000000</td>\n",
       "      <td>30000.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>30000.000000</td>\n",
       "      <td>30000.000000</td>\n",
       "      <td>30000.000000</td>\n",
       "      <td>30000.000000</td>\n",
       "      <td>3.000000e+04</td>\n",
       "      <td>30000.00000</td>\n",
       "      <td>30000.000000</td>\n",
       "      <td>30000.000000</td>\n",
       "      <td>30000.000000</td>\n",
       "      <td>30000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>15000.500000</td>\n",
       "      <td>167484.322667</td>\n",
       "      <td>1.603733</td>\n",
       "      <td>1.853133</td>\n",
       "      <td>1.551867</td>\n",
       "      <td>35.485500</td>\n",
       "      <td>-0.016700</td>\n",
       "      <td>-0.133767</td>\n",
       "      <td>-0.166200</td>\n",
       "      <td>-0.220667</td>\n",
       "      <td>...</td>\n",
       "      <td>43262.948967</td>\n",
       "      <td>40311.400967</td>\n",
       "      <td>38871.760400</td>\n",
       "      <td>5663.580500</td>\n",
       "      <td>5.921163e+03</td>\n",
       "      <td>5225.68150</td>\n",
       "      <td>4826.076867</td>\n",
       "      <td>4799.387633</td>\n",
       "      <td>5215.502567</td>\n",
       "      <td>0.221200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>8660.398374</td>\n",
       "      <td>129747.661567</td>\n",
       "      <td>0.489129</td>\n",
       "      <td>0.790349</td>\n",
       "      <td>0.521970</td>\n",
       "      <td>9.217904</td>\n",
       "      <td>1.123802</td>\n",
       "      <td>1.197186</td>\n",
       "      <td>1.196868</td>\n",
       "      <td>1.169139</td>\n",
       "      <td>...</td>\n",
       "      <td>64332.856134</td>\n",
       "      <td>60797.155770</td>\n",
       "      <td>59554.107537</td>\n",
       "      <td>16563.280354</td>\n",
       "      <td>2.304087e+04</td>\n",
       "      <td>17606.96147</td>\n",
       "      <td>15666.159744</td>\n",
       "      <td>15278.305679</td>\n",
       "      <td>17777.465775</td>\n",
       "      <td>0.415062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-170000.000000</td>\n",
       "      <td>-81334.000000</td>\n",
       "      <td>-339603.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>7500.750000</td>\n",
       "      <td>50000.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>2326.750000</td>\n",
       "      <td>1763.000000</td>\n",
       "      <td>1256.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>8.330000e+02</td>\n",
       "      <td>390.00000</td>\n",
       "      <td>296.000000</td>\n",
       "      <td>252.500000</td>\n",
       "      <td>117.750000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>15000.500000</td>\n",
       "      <td>140000.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>19052.000000</td>\n",
       "      <td>18104.500000</td>\n",
       "      <td>17071.000000</td>\n",
       "      <td>2100.000000</td>\n",
       "      <td>2.009000e+03</td>\n",
       "      <td>1800.00000</td>\n",
       "      <td>1500.000000</td>\n",
       "      <td>1500.000000</td>\n",
       "      <td>1500.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>22500.250000</td>\n",
       "      <td>240000.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>41.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>54506.000000</td>\n",
       "      <td>50190.500000</td>\n",
       "      <td>49198.250000</td>\n",
       "      <td>5006.000000</td>\n",
       "      <td>5.000000e+03</td>\n",
       "      <td>4505.00000</td>\n",
       "      <td>4013.250000</td>\n",
       "      <td>4031.500000</td>\n",
       "      <td>4000.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>30000.000000</td>\n",
       "      <td>1000000.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>79.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>891586.000000</td>\n",
       "      <td>927171.000000</td>\n",
       "      <td>961664.000000</td>\n",
       "      <td>873552.000000</td>\n",
       "      <td>1.684259e+06</td>\n",
       "      <td>896040.00000</td>\n",
       "      <td>621000.000000</td>\n",
       "      <td>426529.000000</td>\n",
       "      <td>528666.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 ID       LIMIT_BAL           SEX     EDUCATION      MARRIAGE  \\\n",
       "count  30000.000000    30000.000000  30000.000000  30000.000000  30000.000000   \n",
       "mean   15000.500000   167484.322667      1.603733      1.853133      1.551867   \n",
       "std     8660.398374   129747.661567      0.489129      0.790349      0.521970   \n",
       "min        1.000000    10000.000000      1.000000      0.000000      0.000000   \n",
       "25%     7500.750000    50000.000000      1.000000      1.000000      1.000000   \n",
       "50%    15000.500000   140000.000000      2.000000      2.000000      2.000000   \n",
       "75%    22500.250000   240000.000000      2.000000      2.000000      2.000000   \n",
       "max    30000.000000  1000000.000000      2.000000      6.000000      3.000000   \n",
       "\n",
       "                AGE         PAY_0         PAY_2         PAY_3         PAY_4  \\\n",
       "count  30000.000000  30000.000000  30000.000000  30000.000000  30000.000000   \n",
       "mean      35.485500     -0.016700     -0.133767     -0.166200     -0.220667   \n",
       "std        9.217904      1.123802      1.197186      1.196868      1.169139   \n",
       "min       21.000000     -2.000000     -2.000000     -2.000000     -2.000000   \n",
       "25%       28.000000     -1.000000     -1.000000     -1.000000     -1.000000   \n",
       "50%       34.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "75%       41.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "max       79.000000      8.000000      8.000000      8.000000      8.000000   \n",
       "\n",
       "       ...      BILL_AMT4      BILL_AMT5      BILL_AMT6       PAY_AMT1  \\\n",
       "count  ...   30000.000000   30000.000000   30000.000000   30000.000000   \n",
       "mean   ...   43262.948967   40311.400967   38871.760400    5663.580500   \n",
       "std    ...   64332.856134   60797.155770   59554.107537   16563.280354   \n",
       "min    ... -170000.000000  -81334.000000 -339603.000000       0.000000   \n",
       "25%    ...    2326.750000    1763.000000    1256.000000    1000.000000   \n",
       "50%    ...   19052.000000   18104.500000   17071.000000    2100.000000   \n",
       "75%    ...   54506.000000   50190.500000   49198.250000    5006.000000   \n",
       "max    ...  891586.000000  927171.000000  961664.000000  873552.000000   \n",
       "\n",
       "           PAY_AMT2      PAY_AMT3       PAY_AMT4       PAY_AMT5  \\\n",
       "count  3.000000e+04   30000.00000   30000.000000   30000.000000   \n",
       "mean   5.921163e+03    5225.68150    4826.076867    4799.387633   \n",
       "std    2.304087e+04   17606.96147   15666.159744   15278.305679   \n",
       "min    0.000000e+00       0.00000       0.000000       0.000000   \n",
       "25%    8.330000e+02     390.00000     296.000000     252.500000   \n",
       "50%    2.009000e+03    1800.00000    1500.000000    1500.000000   \n",
       "75%    5.000000e+03    4505.00000    4013.250000    4031.500000   \n",
       "max    1.684259e+06  896040.00000  621000.000000  426529.000000   \n",
       "\n",
       "            PAY_AMT6  default.payment.next.month  \n",
       "count   30000.000000                30000.000000  \n",
       "mean     5215.502567                    0.221200  \n",
       "std     17777.465775                    0.415062  \n",
       "min         0.000000                    0.000000  \n",
       "25%       117.750000                    0.000000  \n",
       "50%      1500.000000                    0.000000  \n",
       "75%      4000.000000                    0.000000  \n",
       "max    528666.000000                    1.000000  \n",
       "\n",
       "[8 rows x 25 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 30000 entries, 0 to 29999\n",
      "Data columns (total 25 columns):\n",
      " #   Column                      Non-Null Count  Dtype  \n",
      "---  ------                      --------------  -----  \n",
      " 0   ID                          30000 non-null  int64  \n",
      " 1   LIMIT_BAL                   30000 non-null  float64\n",
      " 2   SEX                         30000 non-null  int64  \n",
      " 3   EDUCATION                   30000 non-null  int64  \n",
      " 4   MARRIAGE                    30000 non-null  int64  \n",
      " 5   AGE                         30000 non-null  int64  \n",
      " 6   PAY_0                       30000 non-null  int64  \n",
      " 7   PAY_2                       30000 non-null  int64  \n",
      " 8   PAY_3                       30000 non-null  int64  \n",
      " 9   PAY_4                       30000 non-null  int64  \n",
      " 10  PAY_5                       30000 non-null  int64  \n",
      " 11  PAY_6                       30000 non-null  int64  \n",
      " 12  BILL_AMT1                   30000 non-null  float64\n",
      " 13  BILL_AMT2                   30000 non-null  float64\n",
      " 14  BILL_AMT3                   30000 non-null  float64\n",
      " 15  BILL_AMT4                   30000 non-null  float64\n",
      " 16  BILL_AMT5                   30000 non-null  float64\n",
      " 17  BILL_AMT6                   30000 non-null  float64\n",
      " 18  PAY_AMT1                    30000 non-null  float64\n",
      " 19  PAY_AMT2                    30000 non-null  float64\n",
      " 20  PAY_AMT3                    30000 non-null  float64\n",
      " 21  PAY_AMT4                    30000 non-null  float64\n",
      " 22  PAY_AMT5                    30000 non-null  float64\n",
      " 23  PAY_AMT6                    30000 non-null  float64\n",
      " 24  default.payment.next.month  30000 non-null  int64  \n",
      "dtypes: float64(13), int64(12)\n",
      "memory usage: 5.7 MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#['LIMIT_BAL', 'SEX', 'EDUCATION', 'MARRIAGE', 'AGE', 'PAY_0', 'PAY_2', 'PAY_3', 'PAY_4', 'PAY_5']\n",
    "features = df.columns[1:11].tolist()\n",
    "\n",
    "##[[ 2.0e+04  2.0e+00  2.0e+00 ... -1.0e+00 -1.0e+00 -2.0e+00]\n",
    "# [ 1.2e+05  2.0e+00  2.0e+00 ...  0.0e+00  0.0e+00  0.0e+00]\n",
    "borrower_features = df[features].values\n",
    "\n",
    "\n",
    "#Converts the given value(borower_features) to a Tensor\n",
    "borrower_features = tf.convert_to_tensor(borrower_features, np.float32)\n",
    "\n",
    "#Creates a constant tensor from a tensor-like object\n",
    "idx = tf.constant(list(range(0,100)))\n",
    "\n",
    "#Gather slices from params axis axis according to indices.\n",
    "borrower_features = tf.gather(borrower_features, idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " shape of dense1:  (100, 7)\n",
      "\n",
      " shape of dense2:  (100, 3)\n",
      "\n",
      " shape of predictions:  (100, 1)\n"
     ]
    }
   ],
   "source": [
    "# Define the first dense layer\n",
    "dense1 = tf.keras.layers.Dense(7, activation='sigmoid')(borrower_features)\n",
    "\n",
    "# Define a dense layer with 3 output nodes\n",
    "dense2 = tf.keras.layers.Dense(3, activation='sigmoid')(dense1)\n",
    "\n",
    "# Define a dense layer with 1 output node\n",
    "predictions = tf.keras.layers.Dense(1, activation='sigmoid')(dense2)\n",
    "\n",
    "# Print the shapes of dense1, dense2, and predictions\n",
    "print('\\n shape of dense1: ', dense1.shape)\n",
    "print('\\n shape of dense2: ', dense2.shape)\n",
    "print('\\n shape of predictions: ', predictions.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With just 8 lines of code, you were able to define 2 dense hidden layers and an output layer. This is the advantage of using high-level operations in tensorflow. Note that each layer has 100 rows because the input data contains 100 examples.\n",
    "\n",
    "##### Activation functions\n",
    "Activation function\n",
    "Component of a typical hidden layer\n",
    "Linear: Matrix multiplication\n",
    "Nonlinear: Activation function\n",
    "Binary classification problems\n",
    "In this exercise, you will again make use of credit card data. The target variable, default, indicates whether a credit card holder defaults on his or her payment in the following period. Since there are only two options--default or not--this is a binary classification problem. While the dataset has many features, you will focus on just three: the size of the three latest credit card bills. Finally, you will compute predictions from your untrained network, outputs, and compare those the target variable, default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "bill_amounts = df[['BILL_AMT1', 'BILL_AMT2', 'BILL_AMT3']].to_numpy()\n",
    "default = df[['default.payment.next.month']].to_numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n"
     ]
    }
   ],
   "source": [
    "# Construct input layer from features\n",
    "inputs = tf.constant(bill_amounts, tf.float32)\n",
    "\n",
    "# Define first dense layer\n",
    "dense1 = tf.keras.layers.Dense(3, activation='relu')(inputs)\n",
    "\n",
    "# Define second dense layer\n",
    "dense2 = tf.keras.layers.Dense(2, activation='relu')(dense1)\n",
    "\n",
    "# Define output layer\n",
    "outputs = tf.keras.layers.Dense(1, activation='sigmoid')(dense2)\n",
    "\n",
    "# Print error for first five examples\n",
    "error = default[:5] - outputs.numpy()[:5]\n",
    "print(error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you run the code several times, you'll notice that the errors change each time. This is because you're using an untrained model with randomly initialized parameters. Furthermore, the errors fall on the interval between -1 and 1 because default is a binary variable that takes on values of 0 and 1 and outputs is a probability between 0 and 1.\n",
    "\n",
    "##### Multiclass classification problems\n",
    "In this exercise, we expand beyond binary classification to cover multiclass problems. A multiclass problem has targets that can take on three or more values. In the credit card dataset, the education variable can take on 6 different values, each corresponding to a different level of education. We will use that as our target in this exercise and will also expand the feature set from 3 to 10 columns.\n",
    "\n",
    "As in the previous problem, you will define an input layer, dense layers, and an output layer. You will also print the untrained model's predictions, which are probabilities assigned to the classes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.07261349 0.04243592 0.45688495 0.05277433 0.02321015 0.35208112]\n",
      " [0.07261349 0.04243592 0.45688495 0.05277433 0.02321015 0.35208112]\n",
      " [0.07261349 0.04243592 0.45688495 0.05277433 0.02321015 0.35208112]]\n"
     ]
    }
   ],
   "source": [
    "features = df.columns[1:11].tolist()\n",
    "borrower_features = df[features].values\n",
    "\n",
    "\n",
    "inputs = tf.constant(borrower_features, tf.float32)\n",
    "\n",
    "# Define first dense layer\n",
    "dense1 = tf.keras.layers.Dense(10, activation='sigmoid')(inputs)\n",
    "\n",
    "# Define second dense layer\n",
    "dense2 = tf.keras.layers.Dense(8, activation='relu')(dense1)\n",
    "\n",
    "# Define output layer\n",
    "outputs = tf.keras.layers.Dense(6, activation='softmax')(dense2)\n",
    "\n",
    "# Print first five predictions\n",
    "print(outputs.numpy()[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that each row of outputs sums to one. This is because a row contains the predicted class probabilities for one example. As with the previous exercise, our predictions are not yet informative, since we are using an untrained model with randomly initialized parameters. This is why the model tends to assign similar probabilities to each class.\n",
    "\n",
    "\n",
    "##### Optimizers\n",
    "Stochastic Gradient Descent (SGD) optimizer\n",
    "Simple and easy to interpret\n",
    "Root Mean Squared (RMS) propagation optimizer\n",
    "Applies different learning rates to each feature\n",
    "Allows for momentum to both build and decay\n",
    "Adaptive Momemtum (Adam) optimizer\n",
    "performs well with default parameter values\n",
    "The dangers of local minima\n",
    "Consider the plot of the following loss function, loss_function(), which contains a global minimum, marked by the dot on the right, and several local minima, including the one marked by the dot on the left.\n",
    "\n",
    "In this exercise, you will try to find the global minimum of loss_function() using keras.optimizers.SGD(). You will do this twice, each time with a different initial value of the input to loss_function(). First, you will use x_1, which is a variable with an initial value of 6.0. Second, you will use x_2, which is a variable with an initial value of 0.3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.027515 0.25\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "def loss_function(x):\n",
    "    return 4.0 * math.cos(x - 1) + math.cos(2.0 * math.pi * x) / x\n",
    "\n",
    "# Initialize x_1 and x_2\n",
    "x_1 = tf.Variable(6.0, tf.float32)\n",
    "x_2 = tf.Variable(0.3, tf.float32)\n",
    "\n",
    "# Define the optimization operation\n",
    "opt = tf.keras.optimizers.SGD(learning_rate=0.01)\n",
    "\n",
    "for j in range(100):\n",
    "    # Perform minimization using the loss function and x_1\n",
    "    opt.minimize(lambda: loss_function(x_1), var_list=[x_1])\n",
    "    \n",
    "    # Perform minimization using the loss function and x_2\n",
    "    opt.minimize(lambda: loss_function(x_2), var_list=[x_2])\n",
    "\n",
    "# Print x_1 and x_2 as numpy arrays\n",
    "print(x_1.numpy(), x_2.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we used the same optimizer and loss function, but two different initial values. When we started at 6.0 with x_1, we found the global minimum at 6.03(?), marked by the dot on the right. When we started at 0.3, we stopped around 0.25 with x_2, the local minimum marked by a dot on the far left.\n",
    "\n",
    "Avoiding local minima\n",
    "The previous problem showed how easy it is to get stuck in local minima. We had a simple optimization problem in one variable and gradient descent still failed to deliver the global minimum when we had to travel through local minima first. One way to avoid this problem is to use momentum, which allows the optimizer to break through local minima. We will again use the loss function from the previous problem, which has been defined and is available for you as loss_function().\n",
    "\n",
    "Several optimizers in tensorflow have a momentum parameter, including SGD and RMSprop. You will make use of RMSprop in this exercise. Note that x_1 and x_2 have been initialized to the same value this time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.7445114 0.24999999\n"
     ]
    }
   ],
   "source": [
    "# Initialize x_1 and x_2\n",
    "x_1 = tf.Variable(0.05, tf.float32)\n",
    "x_2 = tf.Variable(0.05, tf.float32)\n",
    "\n",
    "# Define the optimization operation for opt_1 and opt_2\n",
    "opt_1 = tf.keras.optimizers.RMSprop(learning_rate=0.01, momentum=0.99)\n",
    "opt_2 = tf.keras.optimizers.RMSprop(learning_rate=0.01, momentum=0.00)\n",
    "\n",
    "for j in range(100):\n",
    "    opt_1.minimize(lambda: loss_function(x_1), var_list=[x_1])\n",
    "    # Define the minimization operation for opt_2\n",
    "    opt_2.minimize(lambda: loss_function(x_2), var_list=[x_2])\n",
    "    \n",
    "# Print x_1 and x_2 as numpy arrays\n",
    "print(x_1.numpy(), x_2.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that the global minimum is approximately 4.38. Notice that opt_1 built momentum, bringing x_1 closer to the global minimum. To the contrary, opt_2, which had a momentum parameter of 0.0, got stuck in the local minimum on the left.\n",
    "\n",
    "##### Training a network in TensorFlow\n",
    "Random Initializers\n",
    "Often need to initialize thousands of variables\n",
    "tf.ones() may perform poorly\n",
    "Tedious and difficult to initialize variables individually\n",
    "Alternatively, draw initial values from distribution\n",
    "Normal\n",
    "Uniform\n",
    "Glorot initializer\n",
    "Applying dropout\n",
    "\n",
    "##### Initialization in TensorFlow\n",
    "A good initialization can reduce the amount of time needed to find the global minimum. In this exercise, we will initialize weights and biases for a neural network that will be used to predict credit card default decisions. To build intuition, we will use the low-level, linear algebraic approach, rather than making use of convenience functions and high-level keras operations. We will also expand the set of input features from 3 to 23.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1 = tf.Variable(tf.random.normal([23, 7]), tf.float32)\n",
    "\n",
    "# Initialize the layer 1 bias\n",
    "b1 = tf.Variable(tf.ones([7]), tf.float32)\n",
    "\n",
    "# Define the layer 2 weights\n",
    "w2 = tf.Variable(tf.random.normal([7, 1]), tf.float32)\n",
    "\n",
    "# Define the layer 2 bias\n",
    "b2 = tf.Variable(0.0, tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the model and loss function\n",
    "In this exercise, you will train a neural network to predict whether a credit card holder will default. The features and targets you will use to train your network are available in the Python shell as borrower_features and default. You defined the weights and biases in the previous exercise.\n",
    "\n",
    "Note that the predictions layer is defined as \n",
    "σ(layer1  * w2 + b2), where \n",
    "σ is the sigmoid activation, layer1 is a tensor of nodes for the first hidden dense layer, w2 is a tensor of weights, and b2 is the bias tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "df.head()\n",
    "\n",
    "X = df.iloc[:3000 ,1:24].astype(np.float32).to_numpy()\n",
    "y = df.iloc[:3000, 24].astype(np.float32).to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 0., ..., 0., 1., 0.], dtype=float32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "borrower_features, test_features, borrower_targets, test_targets = train_test_split(X, \n",
    "                                                                                    y, \n",
    "                                                                                    test_size=0.25,\n",
    "                                                                                   stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "def model(w1, b1, w2, b2, features=borrower_features):\n",
    "    # Apply relu activation function to layer 1\n",
    "    layer1 = tf.keras.activations.relu(tf.matmul(features, w1) + b1)\n",
    "    \n",
    "    # Apply Dropout\n",
    "    dropout = tf.keras.layers.Dropout(0.25)(layer1)\n",
    "    return tf.keras.activations.sigmoid(tf.matmul(dropout, w2) + b2)\n",
    "\n",
    "# Define the loss function\n",
    "def loss_function(w1, b1, w2, b2, features=borrower_features, targets = borrower_targets):\n",
    "    predictions = model(w1, b1, w2, b2)\n",
    "    \n",
    "    # Pass targets and predictions to the cross entropy loss\n",
    "    return tf.keras.losses.binary_crossentropy(targets, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### raining neural networks with TensorFlow\n",
    "In the previous exercise, you defined a model, model(w1, b1, w2, b2, features), and a loss function, loss_function(w1, b1, w2, b2, features, targets), both of which are available to you in this exercise. You will now train the model and then evaluate its performance by predicting default outcomes in a test set, which consists of test_features and test_targets and is available to you. The trainable variables are w1, b1, w2, and b2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[582,   1],\n",
       "       [167,   0]], dtype=int64)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt = tf.keras.optimizers.Adam(learning_rate=0.1, beta_1=0.9, beta_2=0.999, amsgrad=False)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Train the model\n",
    "for j in range(1000):\n",
    "    # Complete the optimizer\n",
    "    opt.minimize(lambda: loss_function(w1, b1, w2, b2), var_list=[w1, b1, w2, b2])\n",
    "    \n",
    "# Make predictions with model\n",
    "model_predictions = model(w1, b1, w2, b2, test_features)\n",
    "\n",
    "# Construct the confusion matrix\n",
    "confusion_matrix(test_targets.reshape(-1, 1), model_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "####Additional : Plot heatmap\n",
    "import seaborn as sns\n",
    "\n",
    "def confusion_matrix_plot(default, model_predictions):\n",
    "    df = pd.DataFrame(np.hstack([default, model_predictions.numpy() > 0.5]),\n",
    "                      columns = ['Actual','Predicted'])\n",
    "    confusion_matrix = pd.crosstab(df['Actual'], df['Predicted'], \n",
    "                                   rownames=['Actual'], colnames=['Predicted'])\n",
    "    sns.heatmap(confusion_matrix, cmap=\"Greys\", fmt=\"d\", annot=True, cbar=False)\n",
    "confusion_matrix_plot(test_targets.reshape(-1, 1), model_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The diagram shown is called a \"confusion matrix.\" The diagonal elements show the number of correct predictions. The off-diagonal elements show the number of incorrect predictions. We can see that the model performs reasonably-well, but does so by overpredicting non-default. This suggests that we may need to train longer, tune the model's hyperparameters, or change the model's architecture.\n",
    "\n",
    "####  04 - High Level APIs\n",
    "The sequential model in Keras\n",
    "In chapter 3, we used components of the keras API in tensorflow to define a neural network, but we stopped short of using its full capabilities to streamline model definition and training. In this exercise, you will use the keras sequential model API to define a neural network that can be used to classify images of sign language letters. You will also use the .summary() method to print the model's architecture, including the shape and number of parameters associated with each layer.\n",
    "\n",
    "Note that the images were reshaped from (28, 28) to (784,), so that they could be used as inputs to a dense layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 16)                12560     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 4)                 36        \n",
      "=================================================================\n",
      "Total params: 12,732\n",
      "Trainable params: 12,732\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "# Define a Keras sequential model\n",
    "#we'll import the Sequential model type from Keras. This is simply a linear stack of \n",
    "#neural network layers\n",
    "model = tf.keras.Sequential()\n",
    "\n",
    "# Define the first dense layer\n",
    "#Next, let's import the \"core\" layers from Keras. \n",
    "#Each layer of our neural network is defined as a dense layer in keras. \n",
    "#The Dense function takes in 3 parameters. The number of nodes in the layer,\n",
    "#the input shape, and activation function.\n",
    "\n",
    "#The activation function will help us calculate the appropriate weights to make accurate \n",
    "#predictions. There are a number of activation functions to choose from but the most \n",
    "#common are sigmoid tanh and relu\n",
    "#Generally you select an activation function based on the input range. \n",
    "#Sigmoid functions typically deal with outputs ranging from [0,1] while hyptertangent\n",
    "#deals with and output range of [-1,1]. Relu is widely used in convolutional\n",
    "#neural networks because it deals with outputs between [0,infinity].\n",
    "model.add(tf.keras.layers.Dense(16, activation='relu', input_shape=(784,)))\n",
    "\n",
    "# Define the second dense layer\n",
    "model.add(tf.keras.layers.Dense(8, activation='relu', ))\n",
    "\n",
    "# Define the output layer\n",
    "model.add(tf.keras.layers.Dense(4, activation='softmax'))\n",
    "\n",
    "# Print the model architecture\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we've defined a model, but we haven't compiled it. The compilation step in keras allows us to set the optimizer, loss function, and other useful training parameters in a single line of code. Furthermore, the .summary() method allows us to view the model's architecture.\n",
    "\n",
    "##### Compiling a sequential model\n",
    "In this exercise, you will work towards classifying letters from the Sign Language MNIST dataset; however, you will adopt a different network architecture than what you used in the previous exercise. There will be fewer layers, but more nodes. You will also apply dropout to prevent overfitting. Finally, you will compile the model to use the adam optimizer and the categorical_crossentropy loss. You will also use a method in keras to summarize your model's architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_3 (Dense)              (None, 16)                12560     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 4)                 68        \n",
      "=================================================================\n",
      "Total params: 12,628\n",
      "Trainable params: 12,628\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential()\n",
    "\n",
    "# Define the first dense layer\n",
    "model.add(tf.keras.layers.Dense(16, activation='sigmoid', input_shape=(784,)))\n",
    "\n",
    "# Apply dropout to the first layer's output\n",
    "model.add(tf.keras.layers.Dropout(0.25))\n",
    "\n",
    "# Define the output layer\n",
    "model.add(tf.keras.layers.Dense(4, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
    "\n",
    "# Print a model summary\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining a multiple input model\n",
    "In some cases, the sequential API will not be sufficiently flexible to accommodate your desired model architecture and you will need to use the functional API instead. If, for instance, you want to train two models with different architectures jointly, you will need to use the functional API to do this. In this exercise, we will see how to do this. We will also use the .summary() method to examine the joint model's architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 784)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, 784)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 12)           9420        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 12)           9420        input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 4)            52          dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 4)            52          dense_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, 4)            0           dense_6[0][0]                    \n",
      "                                                                 dense_8[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 18,944\n",
      "Trainable params: 18,944\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "m1_inputs = tf.keras.Input(shape=(784,))\n",
    "m2_inputs = tf.keras.Input(shape=(784,))\n",
    "# For model 1, pass the input layer to layer 1 and layer 1 to layer 2\n",
    "m1_layer1 = tf.keras.layers.Dense(12, activation='sigmoid')(m1_inputs)\n",
    "m1_layer2 = tf.keras.layers.Dense(4, activation='softmax')(m1_layer1)\n",
    "\n",
    "# For model 2, pass the input layer to layer 1 and layer 1 to layer 2\n",
    "m2_layer1 = tf.keras.layers.Dense(12, activation='relu')(m2_inputs)\n",
    "m2_layer2 = tf.keras.layers.Dense(4, activation='softmax')(m2_layer1)\n",
    "\n",
    "# Merge model outputs and define a functional model\n",
    "merged = tf.keras.layers.add([m1_layer2, m2_layer2])\n",
    "model = tf.keras.Model(inputs=[m1_inputs, m2_inputs], outputs=merged)\n",
    "\n",
    "# Print a model summary\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the .summary() method yields a new column: connected to. This column tells you how layers connect to each other within the network. We can see that dense_9, for instance, is connected to the input_2 layer. We can also see that the add layer, which merged the two models, connected to both dense_10 and dense_12.\n",
    "\n",
    "##### Training and validation with Keras\n",
    "Training with Keras\n",
    "In this exercise, we return to our sign language letter classification problem. We have 2000 images of four letters--A, B, C, and D--and we want to classify them with a high level of accuracy. We will complete all parts of the problem, including the model definition, compilation, and training.\n",
    "\n",
    "The MNIST database contains 60,000 training images and 10,000 testing images taken from American Census Bureau employees and American high school students [4]. Therefore, in the second line, I have separated these two groups as train and test and also separated the labels and the images. x_train and x_test parts contain greyscale RGB codes (from 0 to 255) while y_train and y_test parts contains labels from 0 to 9 which represents which number they actually are. To visualize these numbers, we can get help from matplotlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: X=(60000, 28, 28), y=(60000,)\n",
      "Test: X=(10000, 28, 28), y=(10000,)\n"
     ]
    }
   ],
   "source": [
    "# example of loading the mnist dataset\n",
    "import tensorflow as tf\n",
    "from keras.datasets import mnist\n",
    "from matplotlib import pyplot\n",
    "# load dataset\n",
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "# summarize loaded dataset\n",
    "print('Train: X=%s, y=%s' % (X_train.shape, y_train.shape))\n",
    "print('Test: X=%s, y=%s' % (X_test.shape, y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: unrecognized arguments: # Only use this if using iPython\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline # Only use this if using iPython\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1d6d4f69f88>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(X_train[7777], cmap='Greys')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1d6d5768348>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(X_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 28)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### data pre-processing\n",
    "\n",
    "You will get (60000, 28, 28). As you might have guessed 60000 represents the number of images in the train dataset and (28, 28) represents the size of the image: 28 x 28 pixel.\n",
    "#### Reshaping and Normalizing the Images\n",
    "Next, we need to reshape our dataset inputs (X_train and X_test) to the shape that our model expects when we train the model. The first number is the number of images (60,000 for X_train and 10,000 for X_test). Then comes the shape of each image (28x28). The last number is 1, which signifies that the images are greyscale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "# Reshaping the array to 4-dims so that it can work with the Keras API\n",
    "#reshape to match keras expectations\n",
    "X_train = X_train.reshape(60000, 28, 28, 1)\n",
    "X_test = X_test.reshape(10000, 28, 28, 1)\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to ‘one-hot-encode’ our target variable. This means that a column will be created for each output category and a binary variable is inputted for each category. For example, we saw that the first image in the dataset is a 5. This means that the sixth number in our array will have a 1 and the rest of the array will be filled with 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "# one-hot encode target column\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 1., 0., 0., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### start modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "#declare sequential model\n",
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model type that we will be using is Sequential. Sequential is the easiest way to build a model in Keras. It allows you to build a model layer by layer.\n",
    "\n",
    "We use the ‘add()’ function to add layers to our model.\n",
    "\n",
    "Our first 2 layers are Conv2D layers. These are convolution layers that will deal with our input images, which are seen as 2-dimensional matrices.\n",
    "\n",
    "64 in the first layer and 32 in the second layer are the number of nodes in each layer. This number can be adjusted to be higher or lower, depending on the size of the dataset. In our case, 64 and 32 work well, so we will stick with this for now.\n",
    "Kernel size is the size of the filter matrix for our convolution. So a kernel size of 3 means we will have a 3x3 filter matrix. Refer back to the introduction and the first image for a refresher on this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense, Conv2D, Dropout, Flatten, MaxPooling2D\n",
    "\n",
    "#declare input layer\n",
    "# shape - (depth, width, height)\n",
    "#first layer  convolutional 64- number of nodes\n",
    "#kernel size is the size of the filter matrix\n",
    "input_shape = (28, 28, 1)\n",
    "model.add(Conv2D(64, kernel_size=3, input_shape=input_shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 26, 26, 64)\n"
     ]
    }
   ],
   "source": [
    "#We can confirm this by printing the shape of the current model output:\n",
    "print(model.output_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Activation is the activation function for the layer. The activation function we will be using for our first 2 layers is the ReLU, or Rectified Linear Activation. This activation function has been proven to work well in neural networks.\n",
    "Our first layer also takes in an input shape. This is the shape of each input image, 28,28,1 as seen earlier on, with the 1 signifying that the images are greyscale.\n",
    "In between the Conv2D layers and the dense layer, there is a ‘Flatten’ layer. Flatten serves as a connection between the convolution and dense layers.\n",
    "‘Dense’ is the layer type we will use in for our output layer. Dense is a standard layer type that is used in many cases for neural networks.\n",
    "We will have 10 nodes in our output layer, one for each possible outcome (0–9).\n",
    "The activation is ‘softmax’. Softmax makes the output sum up to 1 so the output can be interpreted as probabilities. The model will then make its prediction based on which option has the highest probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#second layer\n",
    "model.add(Conv2D(32, kernel_size=3,activation='relu'))\n",
    "#fully connected dense layers\n",
    "#flatten serves as a connection between the convolution and dense layers\n",
    "model.add(Flatten())\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### next step - compile the model\n",
    "Next, we need to compile our model. Compiling the model takes three parameters: optimizer, loss and metrics.\n",
    "\n",
    "\n",
    "The optimizer controls the learning rate. We will be using ‘adam’ as our optmizer. Adam is generally a good optimizer to use for many cases. The adam optimizer adjusts the learning rate throughout training.\n",
    "\n",
    "The learning rate determines how fast the optimal weights for the model are calculated. A smaller learning rate may lead to more accurate weights (up to a certain point), but the time it takes to compute the weights will be longer.\n",
    "\n",
    "We will use ‘categorical_crossentropy’ for our loss function. This is the most common choice for classification. A lower score indicates that the model is performing better.\n",
    "To make things even easier to interpret, we will use the ‘accuracy’ metric to see the accuracy score on the validation set when we train the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compile model using accuracy to measure model performance\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras has a variety of loss functions and out-of-the-box optimizers to choose from.\n",
    "https://keras.io/objectives/  (loss functions)\n",
    "\n",
    "https://keras.io/optimizers/  (optimizers)\n",
    "\n",
    "##### Step 9: Fit model on training data.\n",
    "To fit the model, all we have to do is declare the batch size and number of epochs to train for, then pass in our training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_5 (Conv2D)            (None, 26, 26, 64)        640       \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 24, 24, 32)        18464     \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 18432)             0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                184330    \n",
      "=================================================================\n",
      "Total params: 203,434\n",
      "Trainable params: 203,434\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will train our model. To train, we will use the ‘fit()’ function on our model with the following parameters: training data (train_X), target data (train_y), validation data, and the number of epochs.\n",
    "For our validation data, we will use the test set provided to us in our dataset, which we have split into X_test and y_test.\n",
    "The number of epochs is the number of times the model will cycle through the data. The more epochs we run, the more the model will improve, up to a certain point. After that point, the model will stop improving during each epoch. For our model, we will set the number of epochs to 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "1875/1875 [==============================] - 110s 59ms/step - loss: 0.3001 - accuracy: 0.9398\n",
      "Epoch 2/3\n",
      "1875/1875 [==============================] - 108s 57ms/step - loss: 0.1150 - accuracy: 0.9660\n",
      "Epoch 3/3\n",
      "1875/1875 [==============================] - 112s 60ms/step - loss: 0.0802 - accuracy: 0.9759\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1d6d5a3efc8>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#fit keras model\n",
    "model.fit(X_train, y_train, epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "1875/1875 [==============================] - 114s 61ms/step - loss: 0.0679 - accuracy: 0.9793 - val_loss: 0.1273 - val_accuracy: 0.9705\n",
      "Epoch 2/3\n",
      "1875/1875 [==============================] - 101s 54ms/step - loss: 0.0556 - accuracy: 0.9839 - val_loss: 0.1722 - val_accuracy: 0.9688\n",
      "Epoch 3/3\n",
      "1875/1875 [==============================] - 102s 54ms/step - loss: 0.0547 - accuracy: 0.9856 - val_loss: 0.1943 - val_accuracy: 0.9676\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1d6d0db9dc8>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After 3 epochs, we have gotten to 98.56% accuracy on our validation set. That’s a very good start! Congrats, you have now built a CNN!\n",
    "\n",
    "##### Using our model to make predictions\n",
    "If you want to see the actual predictions that our model has made for the test data, we can use the predict function. The predict function will give an array with 10 numbers. These numbers are the probabilities that the input image represents each digit (0–9). The array index with the highest number represents the model prediction. The sum of each array equals 1 (since each number is a probability).\n",
    "To show this, we will show the predictions for the first 4 images in the test set.\n",
    "Note: If we have new data, we can input our new data into the predict function to see the predictions our model makes on the new data. Since we don’t have any new unseen data, we will show predictions using the test set for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.05402970e-16, 4.19305320e-23, 4.09803533e-06, 2.68988391e-11,\n",
       "        1.47148029e-29, 6.19412169e-22, 2.39054979e-25, 9.99995947e-01,\n",
       "        1.61692427e-15, 2.49075412e-16],\n",
       "       [7.80981911e-15, 7.62787525e-14, 1.00000000e+00, 2.02492279e-16,\n",
       "        4.92680367e-26, 6.46763387e-29, 4.54060852e-16, 9.60333944e-24,\n",
       "        1.53821680e-19, 3.10871012e-24],\n",
       "       [7.67488156e-12, 9.95530546e-01, 1.63340701e-08, 8.61597772e-12,\n",
       "        3.02513649e-06, 1.01571786e-07, 1.38739575e-10, 6.88436287e-13,\n",
       "        4.46641957e-03, 8.20982962e-19],\n",
       "       [1.00000000e+00, 1.52165801e-21, 2.73430079e-09, 1.68681367e-17,\n",
       "        2.12670286e-17, 7.94091588e-16, 2.63392953e-12, 8.11864898e-19,\n",
       "        6.27678465e-13, 1.02973576e-12]], dtype=float32)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#predict first 4 images in the test set\n",
    "model.predict(X_test[:4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "first image:\n",
    "array([[3.05402970e-16, 4.19305320e-23, 4.09803533e-06, 2.68988391e-11,\n",
    "        1.47148029e-29, 6.19412169e-22, 2.39054979e-25, 9.99995947e-01,\n",
    "        1.61692427e-15, 2.49075412e-16]\n",
    "       0,1,2,3,4,5,6,7,8,9\n",
    "       7 is the highest probability\n",
    "second image:\n",
    "[7.80981911e-15, 7.62787525e-14, 1.00000000e+00, 2.02492279e-16,\n",
    "        4.92680367e-26, 6.46763387e-29, 4.54060852e-16, 9.60333944e-24,\n",
    "        1.53821680e-19, 3.10871012e-24],\n",
    "        2 is the highest probability\n",
    "third image:\n",
    "[7.67488156e-12, 9.95530546e-01, 1.63340701e-08, 8.61597772e-12,\n",
    "        3.02513649e-06, 1.01571786e-07, 1.38739575e-10, 6.88436287e-13,\n",
    "        4.46641957e-03, 8.20982962e-19],\n",
    "        \n",
    "1 is the highest probability\n",
    "\n",
    "fourth image:\n",
    "   [1.00000000e+00, 1.52165801e-21, 2.73430079e-09, 1.68681367e-17,\n",
    "        2.12670286e-17, 7.94091588e-16, 2.63392953e-12, 8.11864898e-19,\n",
    "        6.27678465e-13, 1.02973576e-12]], dtype=float32)\n",
    "0 is the highest probability.\n",
    "\n",
    "We can see that our model predicted 7, 2, 1 and 0 for the first four images.\n",
    "Let’s compare this with the actual results.\n",
    "#actual results for first 4 images in test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#actual results for first 4 images in test set\n",
    "y_test[:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The actual results show that the first four images are also 7, 2,1 and 0. Our model predicted correctly!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[7.6748816e-12, 9.9553055e-01, 1.6334070e-08, ..., 6.8843629e-13,\n",
       "        4.4664196e-03, 8.2098296e-19],\n",
       "       [1.0000000e+00, 1.5216580e-21, 2.7343008e-09, ..., 8.1186490e-19,\n",
       "        6.2767846e-13, 1.0297358e-12],\n",
       "       [2.0844360e-20, 2.5034065e-22, 2.7688656e-20, ..., 6.3688671e-22,\n",
       "        6.5284189e-20, 5.7461328e-16],\n",
       "       ...,\n",
       "       [1.0621459e-15, 2.0509320e-11, 2.6937664e-22, ..., 5.4077103e-12,\n",
       "        2.7302360e-09, 1.1493850e-11],\n",
       "       [1.2068821e-16, 4.4114948e-23, 9.2548909e-26, ..., 1.7222342e-18,\n",
       "        9.5421740e-07, 8.4282146e-19],\n",
       "       [1.2072707e-11, 1.1520970e-20, 7.8565201e-12, ..., 8.3525973e-25,\n",
       "        1.1564140e-13, 1.0274610e-25]], dtype=float32)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(X_test[2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test[2:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training models with the Estimators API\n",
    "Estimators API:\n",
    "High-level TensorFlow APIs: Estimators\n",
    "Mid-Level TensorFlow APIs: Layers, Datasets, Metrics\n",
    "Low-level TensorFlow APIs: Python\n",
    "\n",
    "High level submodule\n",
    "Less flexible\n",
    "Faster deployment\n",
    "Many premade model\n",
    "Model specification and training\n",
    "Define feature columns\n",
    "Load and transform data\n",
    "Define an estimator\n",
    "Apply train operation\n",
    "Preparing to train with Estimators\n",
    "For this exercise, we'll return to the King County housing transaction dataset from chapter 2. We will again develop and train a machine learning model to predict house prices; however, this time, we'll do it using the estimator API.\n",
    "\n",
    "Rather than completing everything in one step, we'll break this procedure down into parts. We'll begin by defining the feature columns and loading the data. In the next exercise, we'll define and train a premade estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>price</th>\n",
       "      <th>bedrooms</th>\n",
       "      <th>bathrooms</th>\n",
       "      <th>sqft_living</th>\n",
       "      <th>sqft_lot</th>\n",
       "      <th>floors</th>\n",
       "      <th>waterfront</th>\n",
       "      <th>view</th>\n",
       "      <th>...</th>\n",
       "      <th>grade</th>\n",
       "      <th>sqft_above</th>\n",
       "      <th>sqft_basement</th>\n",
       "      <th>yr_built</th>\n",
       "      <th>yr_renovated</th>\n",
       "      <th>zipcode</th>\n",
       "      <th>lat</th>\n",
       "      <th>long</th>\n",
       "      <th>sqft_living15</th>\n",
       "      <th>sqft_lot15</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7129300520</td>\n",
       "      <td>20141013T000000</td>\n",
       "      <td>221900.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1180</td>\n",
       "      <td>5650</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>1180</td>\n",
       "      <td>0</td>\n",
       "      <td>1955</td>\n",
       "      <td>0</td>\n",
       "      <td>98178</td>\n",
       "      <td>47.5112</td>\n",
       "      <td>-122.257</td>\n",
       "      <td>1340</td>\n",
       "      <td>5650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6414100192</td>\n",
       "      <td>20141209T000000</td>\n",
       "      <td>538000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.25</td>\n",
       "      <td>2570</td>\n",
       "      <td>7242</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>2170</td>\n",
       "      <td>400</td>\n",
       "      <td>1951</td>\n",
       "      <td>1991</td>\n",
       "      <td>98125</td>\n",
       "      <td>47.7210</td>\n",
       "      <td>-122.319</td>\n",
       "      <td>1690</td>\n",
       "      <td>7639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5631500400</td>\n",
       "      <td>20150225T000000</td>\n",
       "      <td>180000.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.00</td>\n",
       "      <td>770</td>\n",
       "      <td>10000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>6</td>\n",
       "      <td>770</td>\n",
       "      <td>0</td>\n",
       "      <td>1933</td>\n",
       "      <td>0</td>\n",
       "      <td>98028</td>\n",
       "      <td>47.7379</td>\n",
       "      <td>-122.233</td>\n",
       "      <td>2720</td>\n",
       "      <td>8062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2487200875</td>\n",
       "      <td>20141209T000000</td>\n",
       "      <td>604000.0</td>\n",
       "      <td>4</td>\n",
       "      <td>3.00</td>\n",
       "      <td>1960</td>\n",
       "      <td>5000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>1050</td>\n",
       "      <td>910</td>\n",
       "      <td>1965</td>\n",
       "      <td>0</td>\n",
       "      <td>98136</td>\n",
       "      <td>47.5208</td>\n",
       "      <td>-122.393</td>\n",
       "      <td>1360</td>\n",
       "      <td>5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1954400510</td>\n",
       "      <td>20150218T000000</td>\n",
       "      <td>510000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1680</td>\n",
       "      <td>8080</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>1680</td>\n",
       "      <td>0</td>\n",
       "      <td>1987</td>\n",
       "      <td>0</td>\n",
       "      <td>98074</td>\n",
       "      <td>47.6168</td>\n",
       "      <td>-122.045</td>\n",
       "      <td>1800</td>\n",
       "      <td>7503</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           id             date     price  bedrooms  bathrooms  sqft_living  \\\n",
       "0  7129300520  20141013T000000  221900.0         3       1.00         1180   \n",
       "1  6414100192  20141209T000000  538000.0         3       2.25         2570   \n",
       "2  5631500400  20150225T000000  180000.0         2       1.00          770   \n",
       "3  2487200875  20141209T000000  604000.0         4       3.00         1960   \n",
       "4  1954400510  20150218T000000  510000.0         3       2.00         1680   \n",
       "\n",
       "   sqft_lot  floors  waterfront  view  ...  grade  sqft_above  sqft_basement  \\\n",
       "0      5650     1.0           0     0  ...      7        1180              0   \n",
       "1      7242     2.0           0     0  ...      7        2170            400   \n",
       "2     10000     1.0           0     0  ...      6         770              0   \n",
       "3      5000     1.0           0     0  ...      7        1050            910   \n",
       "4      8080     1.0           0     0  ...      8        1680              0   \n",
       "\n",
       "   yr_built  yr_renovated  zipcode      lat     long  sqft_living15  \\\n",
       "0      1955             0    98178  47.5112 -122.257           1340   \n",
       "1      1951          1991    98125  47.7210 -122.319           1690   \n",
       "2      1933             0    98028  47.7379 -122.233           2720   \n",
       "3      1965             0    98136  47.5208 -122.393           1360   \n",
       "4      1987             0    98074  47.6168 -122.045           1800   \n",
       "\n",
       "   sqft_lot15  \n",
       "0        5650  \n",
       "1        7639  \n",
       "2        8062  \n",
       "3        5000  \n",
       "4        7503  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "housing = pd.read_csv('kc_house_data.csv')\n",
    "housing.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define feature columns for bedrooms and bathrooms\n",
    "bedrooms = tf.feature_column.numeric_column(\"bedrooms\")\n",
    "bathrooms = tf.feature_column.numeric_column(\"bathrooms\")\n",
    "\n",
    "# Define the list of feature columns\n",
    "feature_list = [bedrooms, bathrooms]\n",
    "\n",
    "def input_fn():\n",
    "    # Define the labels\n",
    "    labels = np.array(housing['price'])\n",
    "    \n",
    "    # Define the features\n",
    "    features = {'bedrooms': np.array(housing['bedrooms']),\n",
    "                'bathrooms': np.array(housing['bathrooms'])}\n",
    "    \n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Defining Estimators\n",
    "In the previous exercise, you defined a list of feature columns, feature_list, and a data input function, input_fn(). In this exercise, you will build on that work by defining an estimator that makes use of input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "WARNING:tensorflow:Using temporary folder as model directory: C:\\Users\\dave_\\AppData\\Local\\Temp\\tmpv2jdq7ag\n",
      "INFO:tensorflow:Using config: {'_model_dir': 'C:\\\\Users\\\\dave_\\\\AppData\\\\Local\\\\Temp\\\\tmpv2jdq7ag', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "WARNING:tensorflow:From C:\\Users\\dave_\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:1666: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:From C:\\Users\\dave_\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "WARNING:tensorflow:Layer dnn is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\dave_\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\optimizer_v2\\adagrad.py:106: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Calling checkpoint listeners before saving checkpoint 0...\n",
      "INFO:tensorflow:Saving checkpoints for 0 into C:\\Users\\dave_\\AppData\\Local\\Temp\\tmpv2jdq7ag\\model.ckpt.\n",
      "INFO:tensorflow:Calling checkpoint listeners after saving checkpoint 0...\n",
      "INFO:tensorflow:loss = 426471500000.0, step = 0\n",
      "INFO:tensorflow:Calling checkpoint listeners before saving checkpoint 1...\n",
      "INFO:tensorflow:Saving checkpoints for 1 into C:\\Users\\dave_\\AppData\\Local\\Temp\\tmpv2jdq7ag\\model.ckpt.\n",
      "INFO:tensorflow:Calling checkpoint listeners after saving checkpoint 1...\n",
      "INFO:tensorflow:Loss for final step: 426471500000.0.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow_estimator.python.estimator.canned.dnn.DNNRegressorV2 at 0x1d6cfba8e88>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the model and set the number of steps\n",
    "model = tf.estimator.DNNRegressor(feature_columns=feature_list, hidden_units=[2,2])\n",
    "model.train(input_fn, steps=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "WARNING:tensorflow:Using temporary folder as model directory: C:\\Users\\dave_\\AppData\\Local\\Temp\\tmpgo2ioeom\n",
      "INFO:tensorflow:Using config: {'_model_dir': 'C:\\\\Users\\\\dave_\\\\AppData\\\\Local\\\\Temp\\\\tmpgo2ioeom', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "WARNING:tensorflow:Layer linear/linear_model is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\dave_\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\feature_column\\feature_column_v2.py:540: Layer.add_variable (from tensorflow.python.keras.engine.base_layer_v1) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.add_weight` method instead.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Calling checkpoint listeners before saving checkpoint 0...\n",
      "INFO:tensorflow:Saving checkpoints for 0 into C:\\Users\\dave_\\AppData\\Local\\Temp\\tmpgo2ioeom\\model.ckpt.\n",
      "INFO:tensorflow:Calling checkpoint listeners after saving checkpoint 0...\n",
      "INFO:tensorflow:loss = 426471500000.0, step = 0\n",
      "INFO:tensorflow:Calling checkpoint listeners before saving checkpoint 2...\n",
      "INFO:tensorflow:Saving checkpoints for 2 into C:\\Users\\dave_\\AppData\\Local\\Temp\\tmpgo2ioeom\\model.ckpt.\n",
      "INFO:tensorflow:Calling checkpoint listeners after saving checkpoint 2...\n",
      "INFO:tensorflow:Loss for final step: 426469920000.0.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow_estimator.python.estimator.canned.linear.LinearRegressorV2 at 0x1d6d265b088>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the model and set the number of steps\n",
    "model = tf.estimator.LinearRegressor(feature_columns=feature_list)\n",
    "model.train(input_fn, steps=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
